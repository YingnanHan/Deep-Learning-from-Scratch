{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ba2f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2961409589076784\n",
      "=== epoch:1, train acc:0.128, test acc:0.161 ===\n",
      "train loss:2.2968722966006356\n",
      "train loss:2.286255117277518\n",
      "train loss:2.273341611286248\n",
      "train loss:2.259674230790612\n",
      "train loss:2.258179901505221\n",
      "train loss:2.2841334648548646\n",
      "train loss:2.28176219817133\n",
      "train loss:2.285180395809623\n",
      "train loss:2.2954435923836454\n",
      "train loss:2.29750194440428\n",
      "train loss:2.2765408144977615\n",
      "train loss:2.286581159973919\n",
      "train loss:2.26150486183312\n",
      "train loss:2.256891828798726\n",
      "train loss:2.2407652425141036\n",
      "train loss:2.228174178998409\n",
      "train loss:2.249916162198312\n",
      "train loss:2.2226914432056604\n",
      "train loss:2.2066551191137322\n",
      "train loss:2.1842000193612505\n",
      "train loss:2.1236910248769694\n",
      "train loss:2.1628611096734556\n",
      "train loss:2.088080823753464\n",
      "train loss:2.0983017500965384\n",
      "train loss:2.0974786982890192\n",
      "train loss:2.073289678841941\n",
      "train loss:2.1259795472331997\n",
      "train loss:2.138413634014763\n",
      "train loss:2.0159027729004704\n",
      "train loss:2.0798774121945165\n",
      "train loss:2.1179693790539664\n",
      "train loss:2.153640901327255\n",
      "train loss:1.9758499308046016\n",
      "train loss:1.9522721326771804\n",
      "train loss:1.9701631217463906\n",
      "train loss:1.9745839911581131\n",
      "train loss:2.021525906914768\n",
      "train loss:2.0459109453460953\n",
      "train loss:1.9607806653993651\n",
      "train loss:1.9545321430542422\n",
      "train loss:1.920856345600385\n",
      "train loss:1.9637484193379695\n",
      "train loss:1.9377465814159671\n",
      "train loss:1.8326495648917114\n",
      "train loss:1.9810314151875006\n",
      "train loss:1.9667844831129857\n",
      "train loss:1.8660601685760272\n",
      "train loss:1.8265739029215238\n",
      "train loss:1.894385855299991\n",
      "train loss:1.8724263188353083\n",
      "train loss:1.8913757406022034\n",
      "train loss:1.850256754770688\n",
      "train loss:1.983174630107512\n",
      "train loss:1.8617972623040604\n",
      "train loss:1.8603159604498456\n",
      "train loss:1.9002464689438456\n",
      "train loss:1.6080170439481256\n",
      "train loss:1.7594847803855518\n",
      "train loss:2.0371392185318222\n",
      "train loss:1.6285348415932535\n",
      "train loss:1.6543898813416904\n",
      "train loss:1.9504094635437452\n",
      "train loss:1.8150480823190032\n",
      "train loss:1.8453370309547343\n",
      "train loss:1.828696723804356\n",
      "train loss:1.8156215516665508\n",
      "train loss:1.7785755263484564\n",
      "train loss:1.690760769364039\n",
      "train loss:1.7799476951645172\n",
      "train loss:1.7235617717922207\n",
      "train loss:1.8630416754716885\n",
      "train loss:1.7487807621079374\n",
      "train loss:1.7314930769107033\n",
      "train loss:1.6954543061731924\n",
      "train loss:1.600588759846873\n",
      "train loss:1.6835039629814812\n",
      "train loss:1.615878280037008\n",
      "train loss:1.7032085257477128\n",
      "train loss:1.604947133440237\n",
      "train loss:1.6015860846692318\n",
      "train loss:1.723200120251135\n",
      "train loss:1.6864490521213003\n",
      "train loss:1.6917034889427258\n",
      "train loss:1.5499782397357942\n",
      "train loss:1.699790278918358\n",
      "train loss:1.6575201992235078\n",
      "train loss:1.761778420394438\n",
      "train loss:1.6683019584188372\n",
      "train loss:1.6161988142726622\n",
      "train loss:1.7254443490263012\n",
      "train loss:1.4922097307713258\n",
      "train loss:1.6930990437384412\n",
      "train loss:1.577704051780408\n",
      "train loss:1.7096951134562572\n",
      "train loss:1.7695472138536479\n",
      "train loss:1.6402593303952309\n",
      "train loss:1.5486755472018232\n",
      "train loss:1.581670699096456\n",
      "train loss:1.6251692144674816\n",
      "train loss:1.6607758749174482\n",
      "train loss:1.5827456169129384\n",
      "train loss:1.5618067649063323\n",
      "train loss:1.6780546511212384\n",
      "train loss:1.5355927902346866\n",
      "train loss:1.6139804489640575\n",
      "train loss:1.6860779242325674\n",
      "train loss:1.665240918219305\n",
      "train loss:1.6003745284323891\n",
      "train loss:1.5036675529259258\n",
      "train loss:1.727239115921627\n",
      "train loss:1.5846566916826255\n",
      "train loss:1.606297632714056\n",
      "train loss:1.5454578528096579\n",
      "train loss:1.8888728872841312\n",
      "train loss:1.646778184192185\n",
      "train loss:1.5942955108817398\n",
      "train loss:1.7958454322333153\n",
      "train loss:1.659076842150103\n",
      "train loss:1.6935156733651389\n",
      "train loss:1.6447459182387028\n",
      "train loss:1.57835771823725\n",
      "train loss:1.6815173844587306\n",
      "train loss:1.7702964445071503\n",
      "train loss:1.6715653306595366\n",
      "train loss:1.5949363357215653\n",
      "train loss:1.6185111215074415\n",
      "train loss:1.3575811982235084\n",
      "train loss:1.7175105073333572\n",
      "train loss:1.5443344427725139\n",
      "train loss:1.4367520529968782\n",
      "train loss:1.416054384273176\n",
      "train loss:1.590044174413249\n",
      "train loss:1.7197208364162693\n",
      "train loss:1.5122771610375703\n",
      "train loss:1.5539470908266575\n",
      "train loss:1.6953603519072324\n",
      "train loss:1.6405667620285624\n",
      "train loss:1.5771617963231044\n",
      "train loss:1.513167102193817\n",
      "train loss:1.5658122991398409\n",
      "train loss:1.4329997927695228\n",
      "train loss:1.6247067527300123\n",
      "train loss:1.446418396324911\n",
      "train loss:1.39541708871406\n",
      "train loss:1.5525107560449711\n",
      "train loss:1.4505943834502815\n",
      "train loss:1.5790207250923454\n",
      "train loss:1.5263740608216907\n",
      "train loss:1.430655879243855\n",
      "train loss:1.5093358814899045\n",
      "train loss:1.5492923956716842\n",
      "train loss:1.3284875789116486\n",
      "train loss:1.5201220184901398\n",
      "train loss:1.4273363446083775\n",
      "train loss:1.5260001075945795\n",
      "train loss:1.469452102074162\n",
      "train loss:1.5183898211054623\n",
      "train loss:1.4529729304056915\n",
      "train loss:1.647919101221531\n",
      "train loss:1.6240071242029817\n",
      "train loss:1.5442173119840357\n",
      "train loss:1.433939942667337\n",
      "train loss:1.444965398252627\n",
      "train loss:1.677775133767634\n",
      "train loss:1.4182955211157624\n",
      "train loss:1.4782418089085965\n",
      "train loss:1.4750588697022549\n",
      "train loss:1.538776658157979\n",
      "train loss:1.454425459832442\n",
      "train loss:1.5196979635408403\n",
      "train loss:1.4375655476737323\n",
      "train loss:1.4417849496350885\n",
      "train loss:1.5385404551941868\n",
      "train loss:1.6780685016318169\n",
      "train loss:1.4592471365837645\n",
      "train loss:1.460057549357577\n",
      "train loss:1.4719131228580227\n",
      "train loss:1.4540242100083236\n",
      "train loss:1.6637803030150748\n",
      "train loss:1.6778612870218927\n",
      "train loss:1.5235264852422754\n",
      "train loss:1.4211247415442734\n",
      "train loss:1.2703367150386788\n",
      "train loss:1.5753830361207193\n",
      "train loss:1.6131992027937778\n",
      "train loss:1.3389647627890244\n",
      "train loss:1.506471314325302\n",
      "train loss:1.2493340688445649\n",
      "train loss:1.4488422510950185\n",
      "train loss:1.4225048719644309\n",
      "train loss:1.29236174897363\n",
      "train loss:1.4580247906685906\n",
      "train loss:1.4805162232826854\n",
      "train loss:1.5176437546168875\n",
      "train loss:1.3756773531391784\n",
      "train loss:1.4834521403405683\n",
      "train loss:1.4115417613305337\n",
      "train loss:1.4297949346966692\n",
      "train loss:1.5055318203175059\n",
      "train loss:1.4200556622830447\n",
      "train loss:1.4768221427197852\n",
      "train loss:1.4736088115935102\n",
      "train loss:1.4698865623761102\n",
      "train loss:1.554070865899297\n",
      "train loss:1.5019832490792007\n",
      "train loss:1.4301514348261057\n",
      "train loss:1.3640120213046287\n",
      "train loss:1.4008852619448473\n",
      "train loss:1.558999830279296\n",
      "train loss:1.3768278691017775\n",
      "train loss:1.4956242554193335\n",
      "train loss:1.273683699166846\n",
      "train loss:1.4611900563840357\n",
      "train loss:1.4727691673860872\n",
      "train loss:1.2762934986297858\n",
      "train loss:1.4863210242822529\n",
      "train loss:1.2399511285094387\n",
      "train loss:1.3644971597401387\n",
      "train loss:1.4449281162212286\n",
      "train loss:1.2701149988306868\n",
      "train loss:1.5263922723881844\n",
      "train loss:1.3497309005719629\n",
      "train loss:1.3775012693546478\n",
      "train loss:1.4215769098868645\n",
      "train loss:1.3193317893224594\n",
      "train loss:1.187151043718218\n",
      "train loss:1.4472216243738598\n",
      "train loss:1.458001032281675\n",
      "train loss:1.4513014499641586\n",
      "train loss:1.395701861252989\n",
      "train loss:1.3094584487216527\n",
      "train loss:1.4457145914754634\n",
      "train loss:1.3291622673198742\n",
      "train loss:1.3770241831887757\n",
      "train loss:1.3300246542069987\n",
      "train loss:1.4819208344294525\n",
      "train loss:1.268215952816537\n",
      "train loss:1.3504327994234118\n",
      "train loss:1.2693570942460242\n",
      "train loss:1.3812135029374581\n",
      "train loss:1.4371911911076338\n",
      "train loss:1.3974883574289396\n",
      "train loss:1.1892684056113854\n",
      "train loss:1.1857880389386277\n",
      "train loss:1.4591604540112284\n",
      "train loss:1.4536771498535084\n",
      "train loss:1.3643691288245328\n",
      "train loss:1.2157920093477186\n",
      "train loss:1.0746523393805887\n",
      "train loss:1.2264078863909376\n",
      "train loss:1.3977193080624568\n",
      "train loss:1.3703686713425407\n",
      "train loss:1.3848634955561898\n",
      "train loss:1.1977636473880013\n",
      "train loss:1.353115972326866\n",
      "train loss:1.4057060166938944\n",
      "train loss:1.3909317646548027\n",
      "train loss:1.3445871223438195\n",
      "train loss:1.3637031105668997\n",
      "train loss:1.4801532877695303\n",
      "train loss:1.3026117726281095\n",
      "train loss:1.2339049702250915\n",
      "train loss:1.2168353663816167\n",
      "train loss:1.359366244376192\n",
      "train loss:1.3762585437743724\n",
      "train loss:1.2798083142066883\n",
      "train loss:1.3169469460663792\n",
      "train loss:1.3517271065838758\n",
      "train loss:1.4773676656376642\n",
      "train loss:1.4048005805223536\n",
      "train loss:1.2920440580786652\n",
      "train loss:1.5409181539305652\n",
      "train loss:1.3743951946504331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.4769703910194798\n",
      "train loss:1.4035137332987984\n",
      "train loss:1.4056413501476839\n",
      "train loss:1.3300103034841024\n",
      "train loss:1.295949813752774\n",
      "train loss:1.5289828165266874\n",
      "train loss:1.588804416767995\n",
      "train loss:1.2753240322372803\n",
      "train loss:1.5660272987418702\n",
      "train loss:1.2720901807080165\n",
      "train loss:1.2491893655729451\n",
      "train loss:1.1475725622695965\n",
      "train loss:1.266967417622813\n",
      "train loss:1.3033100267640807\n",
      "train loss:1.2002615151246068\n",
      "train loss:1.401596832496006\n",
      "train loss:1.267419409517481\n",
      "train loss:1.2036646267613254\n",
      "train loss:1.0984099407786352\n",
      "train loss:1.532994006304327\n",
      "train loss:1.2002762316475697\n",
      "train loss:1.3323962504119373\n",
      "train loss:1.3041433960560351\n",
      "train loss:1.427654972910062\n",
      "train loss:1.279238638808186\n",
      "train loss:1.2389518001334396\n",
      "train loss:1.2824131379753994\n",
      "train loss:1.5533228929840504\n",
      "train loss:1.3917595562802019\n",
      "train loss:1.2167980620748848\n",
      "train loss:1.4001127572637748\n",
      "train loss:1.4092182085941871\n",
      "train loss:1.3373846062544787\n",
      "train loss:1.272661686117846\n",
      "train loss:1.1309812258530008\n",
      "train loss:1.2919048142353815\n",
      "train loss:1.3006540602831018\n",
      "train loss:1.4044789955435797\n",
      "train loss:1.3685216166788428\n",
      "train loss:1.2346100346064615\n",
      "train loss:1.3033498781359727\n",
      "train loss:1.4288199527029293\n",
      "train loss:1.33924331443277\n",
      "train loss:1.2181467404657391\n",
      "train loss:1.320566923217182\n",
      "train loss:1.332724287789277\n",
      "train loss:1.4777531923308507\n",
      "train loss:1.3205774197314684\n",
      "train loss:1.1318513128443124\n",
      "train loss:1.0938327606149025\n",
      "train loss:1.2364000631920882\n",
      "train loss:1.3224954235754354\n",
      "train loss:1.187372226951584\n",
      "train loss:1.269531682533269\n",
      "train loss:1.111725812919448\n",
      "train loss:1.1122511580152956\n",
      "train loss:1.3520629220978475\n",
      "train loss:1.1477124404605894\n",
      "train loss:1.3352943835209756\n",
      "train loss:1.068697409537737\n",
      "train loss:1.3624243560803624\n",
      "train loss:1.203282496823659\n",
      "train loss:1.177122174521962\n",
      "train loss:1.467008467117821\n",
      "train loss:1.379062003439666\n",
      "train loss:1.1014186754067483\n",
      "train loss:1.3782726322190273\n",
      "train loss:1.4431001748564989\n",
      "train loss:1.1717627554680536\n",
      "train loss:1.2118001850527742\n",
      "train loss:1.3517719109087671\n",
      "train loss:1.1686125705321926\n",
      "train loss:1.2469769045862489\n",
      "train loss:1.3582966058020685\n",
      "train loss:1.3478058785343956\n",
      "train loss:1.205808897545846\n",
      "train loss:1.342228412797422\n",
      "train loss:1.2551875606613858\n",
      "train loss:1.3612638716112533\n",
      "train loss:1.2072041967183054\n",
      "train loss:1.0626006006084294\n",
      "train loss:1.542756543126369\n",
      "train loss:1.2703569205532723\n",
      "train loss:1.038407918464519\n",
      "train loss:1.01984492666847\n",
      "train loss:1.2181281944682976\n",
      "train loss:1.3465572390999816\n",
      "train loss:1.124730063149357\n",
      "train loss:1.257447669807\n",
      "train loss:1.2856033581044581\n",
      "train loss:1.242033054277545\n",
      "train loss:1.4231297646442698\n",
      "train loss:1.2909565599344242\n",
      "train loss:1.2379985657222354\n",
      "train loss:1.1531612357475745\n",
      "train loss:1.1939014614238892\n",
      "train loss:1.1549814729228118\n",
      "train loss:1.1479211339330824\n",
      "train loss:1.3577086932039244\n",
      "train loss:0.9893599644789056\n",
      "train loss:1.2760323145688515\n",
      "train loss:1.2500942294657305\n",
      "train loss:1.146701977197863\n",
      "train loss:1.2711263969115405\n",
      "train loss:1.4316970741104784\n",
      "train loss:1.2668843348266583\n",
      "train loss:1.260798633634609\n",
      "train loss:1.334098983291328\n",
      "train loss:1.2627245605906576\n",
      "train loss:1.0982862298220282\n",
      "train loss:1.267693807272392\n",
      "train loss:1.197325262718008\n",
      "train loss:1.2260355784747472\n",
      "train loss:1.1428949293251598\n",
      "train loss:1.1666789559525137\n",
      "train loss:1.117689480596582\n",
      "train loss:1.3557162623934451\n",
      "train loss:1.292902275116375\n",
      "train loss:1.265337490277769\n",
      "train loss:1.1444317953208\n",
      "train loss:1.1670783811708472\n",
      "train loss:1.350305372212141\n",
      "train loss:1.0542499151449503\n",
      "train loss:1.0874197117211906\n",
      "train loss:1.4213738581804132\n",
      "train loss:1.2101939378949558\n",
      "train loss:1.1877821026273039\n",
      "train loss:1.1281461995755266\n",
      "train loss:1.3735645036641122\n",
      "train loss:1.2937218873456433\n",
      "train loss:1.247769743883571\n",
      "train loss:1.279562572568273\n",
      "train loss:1.1714474790154985\n",
      "train loss:1.1674716725079946\n",
      "train loss:1.119846999774481\n",
      "train loss:1.1283741877280982\n",
      "train loss:1.306429310057584\n",
      "train loss:1.0636968624012746\n",
      "train loss:1.20983830918631\n",
      "train loss:1.1660811251812833\n",
      "train loss:1.0909292599347222\n",
      "train loss:1.168196800556444\n",
      "train loss:1.3365732774844568\n",
      "train loss:1.1159040549297452\n",
      "train loss:1.2272850321281503\n",
      "train loss:1.127549735378401\n",
      "train loss:1.2408766225330328\n",
      "train loss:1.1809564423594678\n",
      "train loss:1.1584066487731015\n",
      "train loss:1.18402699337628\n",
      "train loss:1.083321269423494\n",
      "train loss:1.0784247594651175\n",
      "train loss:1.2584546435026898\n",
      "train loss:1.2659239881128876\n",
      "train loss:1.1313049490204714\n",
      "train loss:1.0571084001442532\n",
      "train loss:1.3507256502275522\n",
      "train loss:1.1416115049723963\n",
      "train loss:1.1621598989445379\n",
      "train loss:1.1282616285225666\n",
      "train loss:1.3822065811839404\n",
      "train loss:1.1322811913880784\n",
      "train loss:1.3489102813563556\n",
      "train loss:1.0794763518208208\n",
      "train loss:1.0636269498922823\n",
      "train loss:1.3093655033210576\n",
      "train loss:1.1738824734010298\n",
      "train loss:1.1195433351574107\n",
      "train loss:1.3719467877919787\n",
      "train loss:1.135625696302628\n",
      "train loss:1.1191049720468014\n",
      "train loss:1.113801987144392\n",
      "train loss:0.9096458962894971\n",
      "train loss:1.2024412032633762\n",
      "train loss:1.1733594216289176\n",
      "train loss:1.11702693596997\n",
      "train loss:1.0885426591368423\n",
      "train loss:1.1143559662436195\n",
      "train loss:1.2263125310863707\n",
      "train loss:1.4431833429547012\n",
      "train loss:1.2797899727630992\n",
      "train loss:1.316406710628377\n",
      "train loss:1.1162153025648704\n",
      "train loss:1.0167016605325765\n",
      "train loss:1.3391949830017118\n",
      "train loss:0.9887377979348816\n",
      "train loss:1.1841857740504171\n",
      "train loss:1.0562939885316749\n",
      "train loss:1.2812546126219295\n",
      "train loss:1.2027982906765282\n",
      "train loss:0.9472679511082189\n",
      "train loss:1.1129254241204867\n",
      "train loss:1.3354831956887612\n",
      "train loss:1.1758878559257249\n",
      "train loss:1.0975864262425852\n",
      "train loss:1.365072791264\n",
      "train loss:1.2096074554960625\n",
      "train loss:1.1662880389973005\n",
      "train loss:1.130091741194341\n",
      "train loss:0.936284074164679\n",
      "train loss:1.1609494858814062\n",
      "train loss:1.2940296470113888\n",
      "train loss:1.1511227290195603\n",
      "train loss:1.2144793653633317\n",
      "train loss:1.0669881349303993\n",
      "train loss:1.0759906862482662\n",
      "train loss:1.0336557475925219\n",
      "train loss:1.0461324393884817\n",
      "train loss:1.2044671978815644\n",
      "train loss:1.3197009745168566\n",
      "train loss:1.15228296241036\n",
      "train loss:1.017685531278718\n",
      "train loss:1.1598037547141282\n",
      "train loss:1.2215816702791056\n",
      "train loss:1.2375731423104082\n",
      "train loss:1.285829326695731\n",
      "train loss:0.9646471312885297\n",
      "train loss:1.027056972986632\n",
      "train loss:1.1475862182382315\n",
      "train loss:1.0218410688001398\n",
      "train loss:1.1060551028998058\n",
      "train loss:1.1671160585318456\n",
      "train loss:1.015625699684663\n",
      "train loss:1.1273289961180142\n",
      "train loss:1.1412288580411634\n",
      "train loss:1.1083671987079238\n",
      "train loss:0.960290374050104\n",
      "train loss:1.1474886819785817\n",
      "train loss:1.1665894733192084\n",
      "train loss:1.1166818788267887\n",
      "train loss:1.0677716299344797\n",
      "train loss:1.0399617532020067\n",
      "train loss:1.037844814684749\n",
      "train loss:1.1268357837666403\n",
      "train loss:1.2640645515266338\n",
      "train loss:1.207117377522679\n",
      "train loss:1.0438377177755764\n",
      "train loss:1.328687664504435\n",
      "train loss:1.1897838585762646\n",
      "train loss:1.0467802161392175\n",
      "train loss:1.2607748126598182\n",
      "train loss:1.1504926679277334\n",
      "train loss:1.1606602061009992\n",
      "train loss:1.1730456356603276\n",
      "train loss:1.0052577349833383\n",
      "train loss:1.1213053754765414\n",
      "train loss:1.1212750328978656\n",
      "train loss:1.0784151231655246\n",
      "train loss:1.176683583064404\n",
      "train loss:1.2640626397655284\n",
      "train loss:0.9516464981247527\n",
      "train loss:1.105865633003769\n",
      "train loss:1.1546163531120557\n",
      "train loss:1.1221712762407778\n",
      "train loss:0.8579001867251086\n",
      "train loss:1.0547920892345624\n",
      "train loss:1.1828044557987718\n",
      "train loss:1.0777628371830412\n",
      "train loss:0.999814793659764\n",
      "train loss:0.9899395713621967\n",
      "train loss:1.1707693150046348\n",
      "train loss:1.0409542869508461\n",
      "train loss:1.093585211435997\n",
      "train loss:1.3026502876308745\n",
      "train loss:1.0093769438742768\n",
      "train loss:1.1266174147613233\n",
      "train loss:1.04175068043652\n",
      "train loss:1.2485776817659215\n",
      "train loss:1.0022007902386918\n",
      "train loss:1.1640621713832038\n",
      "train loss:1.071150668351883\n",
      "train loss:1.1324621759644786\n",
      "train loss:1.2385256795550843\n",
      "train loss:1.219813880479598\n",
      "train loss:1.1486781183300543\n",
      "train loss:1.2476005136542787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.073232907976774\n",
      "train loss:1.2387678554736907\n",
      "train loss:1.1661262652205204\n",
      "train loss:1.0661575697500445\n",
      "train loss:1.041211925585654\n",
      "train loss:1.0772248323495315\n",
      "train loss:1.1789863824086373\n",
      "train loss:0.8663375189839678\n",
      "train loss:1.271511216241415\n",
      "train loss:1.1108011234900108\n",
      "train loss:1.029151969188188\n",
      "train loss:1.287848078975656\n",
      "train loss:0.9367293045648765\n",
      "train loss:0.9660590331088283\n",
      "train loss:1.297538106018585\n",
      "train loss:1.2382470122294114\n",
      "train loss:1.0463722475280521\n",
      "train loss:1.2983873665216845\n",
      "train loss:1.088657864923231\n",
      "train loss:1.1257434742096817\n",
      "train loss:1.0892725549818592\n",
      "train loss:0.9924250316953828\n",
      "train loss:1.115867381334903\n",
      "train loss:0.9298638687241652\n",
      "train loss:1.1361034114540005\n",
      "train loss:0.9704601356799263\n",
      "train loss:1.1040417061487207\n",
      "train loss:1.0314545924219833\n",
      "train loss:1.0562892916569597\n",
      "train loss:1.2357903677820483\n",
      "train loss:1.1740499722751057\n",
      "train loss:1.1592755066995661\n",
      "train loss:1.3371755529975646\n",
      "train loss:0.9853085743142578\n",
      "train loss:1.0861573250452135\n",
      "train loss:1.1816065150114199\n",
      "train loss:0.9923828572150104\n",
      "train loss:1.0811075312531375\n",
      "train loss:1.0176620506802896\n",
      "train loss:0.9985437138038894\n",
      "train loss:1.0306014293587622\n",
      "train loss:1.0274999313062445\n",
      "train loss:1.1280133667004364\n",
      "train loss:1.2021056201956193\n",
      "train loss:1.0390674093039727\n",
      "train loss:1.0238935518492422\n",
      "train loss:1.136530298655748\n",
      "train loss:1.0127234622041503\n",
      "train loss:1.1270001329478176\n",
      "train loss:1.3416605564659607\n",
      "train loss:0.9887527953118519\n",
      "=== epoch:2, train acc:0.978, test acc:0.975 ===\n",
      "train loss:1.0076606552876695\n",
      "train loss:1.3134466198452204\n",
      "train loss:1.2237577950689356\n",
      "train loss:1.0696875725255603\n",
      "train loss:1.1762070786705237\n",
      "train loss:1.2124812782236873\n",
      "train loss:0.9080049757344628\n",
      "train loss:1.1159867539612383\n",
      "train loss:1.1110381096860975\n",
      "train loss:0.9297960103386776\n",
      "train loss:1.2752164813989533\n",
      "train loss:1.1524280266397187\n",
      "train loss:1.3679711258411993\n",
      "train loss:1.0535797119032844\n",
      "train loss:1.156945907153447\n",
      "train loss:1.225124213468097\n",
      "train loss:1.2205065887287645\n",
      "train loss:1.0372320805707667\n",
      "train loss:1.3200299240629996\n",
      "train loss:1.0483937195179205\n",
      "train loss:1.1400394546502164\n",
      "train loss:1.025018019372538\n",
      "train loss:1.0518761371503964\n",
      "train loss:1.1511702064220994\n",
      "train loss:1.1121307897379424\n",
      "train loss:1.1522695500936786\n",
      "train loss:0.9710448541521762\n",
      "train loss:1.040500841523987\n",
      "train loss:1.1250734144184555\n",
      "train loss:1.0725282404094567\n",
      "train loss:1.0490634513437511\n",
      "train loss:1.244161130152491\n",
      "train loss:1.1068781578502291\n",
      "train loss:1.1386358212983307\n",
      "train loss:1.1295030668526451\n",
      "train loss:1.2520038268009608\n",
      "train loss:0.9708539830454466\n",
      "train loss:1.0740766468185756\n",
      "train loss:1.246975351955277\n",
      "train loss:1.2086070150197041\n",
      "train loss:1.1510244951295951\n",
      "train loss:1.1106015459238088\n",
      "train loss:1.1660543849969514\n",
      "train loss:1.1877217699069675\n",
      "train loss:1.1425052158421147\n",
      "train loss:1.0412988170773807\n",
      "train loss:1.0569594834264473\n",
      "train loss:1.0320001341735172\n",
      "train loss:1.0012262775257619\n",
      "train loss:1.0530783591493929\n",
      "train loss:1.1263403480864884\n",
      "train loss:1.0096215750851538\n",
      "train loss:1.1083809226039956\n",
      "train loss:1.1842388192582687\n",
      "train loss:1.1998994903578524\n",
      "train loss:1.2444784285457975\n",
      "train loss:1.1365660872830166\n",
      "train loss:1.2569703353799604\n",
      "train loss:1.1195121735718343\n",
      "train loss:1.1991802934210976\n",
      "train loss:1.2203008341747283\n",
      "train loss:1.1926729822105773\n",
      "train loss:1.1725419815416178\n",
      "train loss:1.1822128028996628\n",
      "train loss:1.2420401098431415\n",
      "train loss:1.1443889502098243\n",
      "train loss:1.190540977623242\n",
      "train loss:1.0580411469513304\n",
      "train loss:1.1575533411825267\n",
      "train loss:1.2257523476706913\n",
      "train loss:0.9551474600673058\n",
      "train loss:1.2181634182716323\n",
      "train loss:1.100233253753337\n",
      "train loss:1.31175146197874\n",
      "train loss:1.016910788467052\n",
      "train loss:1.0038376067899382\n",
      "train loss:0.9272448092773666\n",
      "train loss:1.162778111350391\n",
      "train loss:1.0161698725869681\n",
      "train loss:1.0776443506666338\n",
      "train loss:1.1174258308131253\n",
      "train loss:1.1746671464489364\n",
      "train loss:1.1646051887812696\n",
      "train loss:1.0745638025035396\n",
      "train loss:1.031494858653384\n",
      "train loss:1.1093068658786287\n",
      "train loss:0.8893551241751213\n",
      "train loss:0.9266891024417412\n",
      "train loss:1.054703192178119\n",
      "train loss:1.2186318929074742\n",
      "train loss:1.1235789584407816\n",
      "train loss:1.2197838257564961\n",
      "train loss:1.083508227639724\n",
      "train loss:1.1773858949461748\n",
      "train loss:1.0371315959355025\n",
      "train loss:1.0953410802531047\n",
      "train loss:1.1002412654857863\n",
      "train loss:1.0141499017775188\n",
      "train loss:0.9481455151198717\n",
      "train loss:1.1792281590730684\n",
      "train loss:0.9904452538096521\n",
      "train loss:1.1347993043742124\n",
      "train loss:0.9980091621484517\n",
      "train loss:1.230346248755736\n",
      "train loss:0.9916702435003332\n",
      "train loss:1.038926211404503\n",
      "train loss:1.0877798398116052\n",
      "train loss:1.2192727718245087\n",
      "train loss:1.0833987781180117\n",
      "train loss:1.2791459456392895\n",
      "train loss:1.2193884605246037\n",
      "train loss:1.2130683288931556\n",
      "train loss:0.9683237290750575\n",
      "train loss:1.1427499254733104\n",
      "train loss:1.1659265851809408\n",
      "train loss:1.1298157907928081\n",
      "train loss:1.0119917127021565\n",
      "train loss:1.1280301120378549\n",
      "train loss:1.1534700966733766\n",
      "train loss:1.1098441959590117\n",
      "train loss:0.9487723768307876\n",
      "train loss:1.3133664645863246\n",
      "train loss:1.0168127931075532\n",
      "train loss:1.0868418941657674\n",
      "train loss:1.0698423016231813\n",
      "train loss:1.073999384154698\n",
      "train loss:1.1619489627712019\n",
      "train loss:1.1034409166191759\n",
      "train loss:1.395440772929324\n",
      "train loss:1.0144711689151118\n",
      "train loss:1.1804110774256724\n",
      "train loss:1.0766695385986447\n",
      "train loss:1.0967554224060725\n",
      "train loss:1.1398297684140644\n",
      "train loss:0.9509508537584845\n",
      "train loss:1.1393626299022275\n",
      "train loss:0.9486907581769245\n",
      "train loss:0.9374215314753167\n",
      "train loss:1.2951268873044939\n",
      "train loss:0.9345057570919701\n",
      "train loss:0.9849180566991202\n",
      "train loss:1.1658177192855348\n",
      "train loss:1.0388750875923405\n",
      "train loss:1.1885236548369895\n",
      "train loss:0.9486479227829341\n",
      "train loss:1.1588967011470084\n",
      "train loss:1.122785508398547\n",
      "train loss:1.14994609048533\n",
      "train loss:1.0727540007403653\n",
      "train loss:0.93597350328797\n",
      "train loss:1.1356752450435628\n",
      "train loss:0.9853855502539899\n",
      "train loss:1.1096085346244862\n",
      "train loss:1.152690939288811\n",
      "train loss:1.067255512912351\n",
      "train loss:1.159897570138058\n",
      "train loss:0.8854207104770534\n",
      "train loss:0.8767685816112234\n",
      "train loss:1.1381917526019663\n",
      "train loss:1.0596407758663824\n",
      "train loss:1.0587901921798346\n",
      "train loss:1.0646044677345012\n",
      "train loss:0.883190501499724\n",
      "train loss:1.0385315722803838\n",
      "train loss:1.0440590719547584\n",
      "train loss:1.2515016805918713\n",
      "train loss:1.2110523696259348\n",
      "train loss:1.2053265675440028\n",
      "train loss:1.0735285458513648\n",
      "train loss:1.1084454763981142\n",
      "train loss:0.9977626451270942\n",
      "train loss:1.0712383025700989\n",
      "train loss:1.1553433584437545\n",
      "train loss:1.03097831254212\n",
      "train loss:1.1274734984031038\n",
      "train loss:1.0679788052779038\n",
      "train loss:1.124009240276942\n",
      "train loss:0.8676542640548649\n",
      "train loss:1.139630836470773\n",
      "train loss:1.1413564053795857\n",
      "train loss:0.9506822294466042\n",
      "train loss:1.0015295862703848\n",
      "train loss:1.1071589870279015\n",
      "train loss:1.16419093297649\n",
      "train loss:1.0899788147422795\n",
      "train loss:0.9555125488622263\n",
      "train loss:1.1545669224217607\n",
      "train loss:1.1709903474663674\n",
      "train loss:1.0991881221006659\n",
      "train loss:1.0578942878897803\n",
      "train loss:1.041331858979443\n",
      "train loss:1.0096016367593412\n",
      "train loss:1.097484248632295\n",
      "train loss:0.9953745219213816\n",
      "train loss:1.1488161912173558\n",
      "train loss:1.1783499309827195\n",
      "train loss:1.089433754188874\n",
      "train loss:1.1827716644482489\n",
      "train loss:1.0507601564516953\n",
      "train loss:0.989577773417578\n",
      "train loss:1.0596450543278868\n",
      "train loss:1.0017582310861421\n",
      "train loss:0.9376051578904849\n",
      "train loss:1.06844974669528\n",
      "train loss:1.1131921276098589\n",
      "train loss:1.0847085745560792\n",
      "train loss:1.0131821351913803\n",
      "train loss:0.988449520132296\n",
      "train loss:0.9757959779994012\n",
      "train loss:0.9829158314588979\n",
      "train loss:1.0199146473303646\n",
      "train loss:0.9912581788551575\n",
      "train loss:1.118960522098061\n",
      "train loss:1.098543678215863\n",
      "train loss:0.9901921726528156\n",
      "train loss:1.1115129419884366\n",
      "train loss:1.2413058616553128\n",
      "train loss:1.0614740637821418\n",
      "train loss:1.1904437195844795\n",
      "train loss:1.038080031578952\n",
      "train loss:0.9469213136857476\n",
      "train loss:1.0195189784828276\n",
      "train loss:1.1254829708558143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0376378143834066\n",
      "train loss:1.0406943608154935\n",
      "train loss:0.8585229167826027\n",
      "train loss:1.3264938508006718\n",
      "train loss:1.100021452184731\n",
      "train loss:1.081218605854163\n",
      "train loss:0.9893039522698743\n",
      "train loss:1.0601328439813544\n",
      "train loss:0.9857311180137662\n",
      "train loss:0.9865042546835481\n",
      "train loss:0.9931857633861908\n",
      "train loss:0.9043524236276359\n",
      "train loss:1.1836360657963136\n",
      "train loss:0.9227561572202515\n",
      "train loss:1.0101791528916881\n",
      "train loss:1.1692009962608312\n",
      "train loss:1.0671854778945278\n",
      "train loss:1.0027085224410819\n",
      "train loss:1.0133633378929645\n",
      "train loss:1.057022990111736\n",
      "train loss:1.0944131853140098\n",
      "train loss:1.0685912952662313\n",
      "train loss:1.0031903245058214\n",
      "train loss:0.9602973096886538\n",
      "train loss:1.1519454922394443\n",
      "train loss:0.962695219140464\n",
      "train loss:0.9563779622105193\n",
      "train loss:1.0781460185310974\n",
      "train loss:1.0590948376517684\n",
      "train loss:0.9326571243789524\n",
      "train loss:1.264966402682511\n",
      "train loss:1.0327622254925897\n",
      "train loss:1.048234523483428\n",
      "train loss:0.965503295041049\n",
      "train loss:0.9836366244789313\n",
      "train loss:1.0562404133078338\n",
      "train loss:1.0214102748960785\n",
      "train loss:1.2246063391339148\n",
      "train loss:0.9203852364057954\n",
      "train loss:1.0978213136189656\n",
      "train loss:1.0180353691777564\n",
      "train loss:0.9777394862415746\n",
      "train loss:1.051628955034956\n",
      "train loss:1.0911638256441005\n",
      "train loss:1.0516649218851541\n",
      "train loss:1.036166817248267\n",
      "train loss:0.9866601731758731\n",
      "train loss:1.0814203272136595\n",
      "train loss:1.2445482302643032\n",
      "train loss:0.9940465587435291\n",
      "train loss:1.0152507669052016\n",
      "train loss:1.0186102339820755\n",
      "train loss:1.049576472984389\n",
      "train loss:1.0551922045867026\n",
      "train loss:0.9492668555410928\n",
      "train loss:1.0296987896917549\n",
      "train loss:0.9556152258047331\n",
      "train loss:1.0079502536503828\n",
      "train loss:1.0428779330581444\n",
      "train loss:1.0159581389449805\n",
      "train loss:1.0908795056958092\n",
      "train loss:1.0555346565190433\n",
      "train loss:1.0932816862710593\n",
      "train loss:0.9308736645672145\n",
      "train loss:1.0059407394691589\n",
      "train loss:1.0322609037708448\n",
      "train loss:0.990409112778238\n",
      "train loss:1.014260245097419\n",
      "train loss:0.9453062638637993\n",
      "train loss:1.0364682173045645\n",
      "train loss:1.1467224441827717\n",
      "train loss:1.1713421454778945\n",
      "train loss:1.2013330479964808\n",
      "train loss:1.2982495013758457\n",
      "train loss:1.093008575251401\n",
      "train loss:1.0968670252827575\n",
      "train loss:1.0159566273963607\n",
      "train loss:1.195415610083485\n",
      "train loss:0.9929253230155815\n",
      "train loss:0.8871667917192156\n",
      "train loss:0.9684666557377848\n",
      "train loss:1.2095050670283782\n",
      "train loss:0.9180678495723908\n",
      "train loss:1.0055712895747342\n",
      "train loss:0.9274710897388104\n",
      "train loss:1.0149790264860479\n",
      "train loss:1.0310652154458608\n",
      "train loss:1.091553002092182\n",
      "train loss:1.1670978871188722\n",
      "train loss:1.098799472739107\n",
      "train loss:0.9577955479587608\n",
      "train loss:0.9678588174945116\n",
      "train loss:1.161630257637568\n",
      "train loss:0.9236050198335646\n",
      "train loss:0.9222231563143687\n",
      "train loss:0.9327128749807343\n",
      "train loss:1.040901970084473\n",
      "train loss:1.0922551279701154\n",
      "train loss:0.8566261596674966\n",
      "train loss:1.0368817668975803\n",
      "train loss:1.0825792531762617\n",
      "train loss:1.016031640741609\n",
      "train loss:1.1774192874575213\n",
      "train loss:0.8476458514015629\n",
      "train loss:1.2256157906746745\n",
      "train loss:1.0419709414795069\n",
      "train loss:1.1096224620947495\n",
      "train loss:1.0957099276607003\n",
      "train loss:1.120264800562343\n",
      "train loss:1.033959155137437\n",
      "train loss:0.9858259861791132\n",
      "train loss:1.2293052649987593\n",
      "train loss:1.0803713914585098\n",
      "train loss:1.0003021491307207\n",
      "train loss:1.0050351804758497\n",
      "train loss:1.027336055766137\n",
      "train loss:1.0612016426483646\n",
      "train loss:1.066220158663516\n",
      "train loss:0.9055433654888938\n",
      "train loss:0.8473603238500347\n",
      "train loss:1.2756686480737343\n",
      "train loss:0.8439044753210836\n",
      "train loss:0.9793185894444761\n",
      "train loss:0.7790652605320868\n",
      "train loss:1.0422189267610056\n",
      "train loss:1.090812239881049\n",
      "train loss:0.9300756931562382\n",
      "train loss:0.8976348946489705\n",
      "train loss:0.8953061687994271\n",
      "train loss:0.9207401532903637\n",
      "train loss:1.1065990887135855\n",
      "train loss:0.9897252936458288\n",
      "train loss:1.0076333426520518\n",
      "train loss:1.073937914342816\n",
      "train loss:1.0281078068994731\n",
      "train loss:1.1578383409713908\n",
      "train loss:1.1608130060075164\n",
      "train loss:0.9525200381571226\n",
      "train loss:0.9149734400481795\n",
      "train loss:0.891604726027785\n",
      "train loss:1.1545203360526934\n",
      "train loss:0.9107670864177849\n",
      "train loss:1.2490461669923212\n",
      "train loss:0.9662723534618047\n",
      "train loss:1.0558669287203684\n",
      "train loss:1.0523861531838956\n",
      "train loss:0.98023976390779\n",
      "train loss:0.9389473750995085\n",
      "train loss:1.1133798745741101\n",
      "train loss:1.0789370790577297\n",
      "train loss:0.9955565816848677\n",
      "train loss:1.1887044696142643\n",
      "train loss:1.0390809601823834\n",
      "train loss:1.0870447790084947\n",
      "train loss:1.081322264466201\n",
      "train loss:0.8941304477092393\n",
      "train loss:0.9862558253349036\n",
      "train loss:1.0522210021539582\n",
      "train loss:0.9432414333997423\n",
      "train loss:1.2268708774135442\n",
      "train loss:1.0673898015828418\n",
      "train loss:1.0847470458143413\n",
      "train loss:0.9941179438714209\n",
      "train loss:0.9719584547778963\n",
      "train loss:1.1048233146302189\n",
      "train loss:0.927082815879113\n",
      "train loss:1.0186396568407141\n",
      "train loss:1.1710867951767034\n",
      "train loss:0.9609618394578083\n",
      "train loss:1.203104315216865\n",
      "train loss:0.92970495478121\n",
      "train loss:1.0054826142287752\n",
      "train loss:1.1992214760177267\n",
      "train loss:0.8693635155481519\n",
      "train loss:1.0592600992005154\n",
      "train loss:1.123809787506888\n",
      "train loss:1.0528405361492552\n",
      "train loss:0.9333209094167715\n",
      "train loss:1.060704804874802\n",
      "train loss:0.9988384559237808\n",
      "train loss:1.1869536878191722\n",
      "train loss:1.0866524010732797\n",
      "train loss:0.9567488853577284\n",
      "train loss:1.3173701437756755\n",
      "train loss:1.0025717243436494\n",
      "train loss:1.0623850354744357\n",
      "train loss:1.0592700861270175\n",
      "train loss:0.9780169187080933\n",
      "train loss:1.0145695254162959\n",
      "train loss:1.0186499992528513\n",
      "train loss:1.0937078813642052\n",
      "train loss:0.9876592225391196\n",
      "train loss:1.189341612071732\n",
      "train loss:0.9154960710689218\n",
      "train loss:0.8710755464316976\n",
      "train loss:1.0224984866562183\n",
      "train loss:1.057465486837452\n",
      "train loss:0.9677743804142586\n",
      "train loss:1.070296141088544\n",
      "train loss:1.0099733309267418\n",
      "train loss:0.9703461990522836\n",
      "train loss:0.9008141383022165\n",
      "train loss:1.1116675243437923\n",
      "train loss:1.1409451422100263\n",
      "train loss:0.9757489895508007\n",
      "train loss:1.0406845787603871\n",
      "train loss:1.0389682133379048\n",
      "train loss:1.0079482527755494\n",
      "train loss:1.0662234237120167\n",
      "train loss:0.8824776038570533\n",
      "train loss:1.111473275385404\n",
      "train loss:1.0972931810961617\n",
      "train loss:1.0359772124834619\n",
      "train loss:1.010329668769887\n",
      "train loss:1.0204723690681512\n",
      "train loss:0.9725702644339748\n",
      "train loss:1.1430527166468314\n",
      "train loss:1.0013459793418573\n",
      "train loss:0.8163269786323605\n",
      "train loss:0.855784034645388\n",
      "train loss:0.9452590232680892\n",
      "train loss:0.9876451457991914\n",
      "train loss:0.9317181191402841\n",
      "train loss:1.11650863477516\n",
      "train loss:1.1491710347216522\n",
      "train loss:0.8869690707033961\n",
      "train loss:0.984912724435326\n",
      "train loss:0.9326756193395994\n",
      "train loss:1.0983945437559341\n",
      "train loss:1.079121219586293\n",
      "train loss:1.0898455117014594\n",
      "train loss:0.9320816506755544\n",
      "train loss:0.9531417901594393\n",
      "train loss:1.2511060789846242\n",
      "train loss:0.9859323057390935\n",
      "train loss:1.1566029115485132\n",
      "train loss:1.0260591392501694\n",
      "train loss:1.1060553037681553\n",
      "train loss:0.9805735786001906\n",
      "train loss:1.128320979953342\n",
      "train loss:1.0821428983972579\n",
      "train loss:1.1034686091812325\n",
      "train loss:0.9915806520767987\n",
      "train loss:1.1529873789255909\n",
      "train loss:1.0182706367447034\n",
      "train loss:0.9497189187788468\n",
      "train loss:0.9343440070498545\n",
      "train loss:0.8336004568664981\n",
      "train loss:0.8890878846303407\n",
      "train loss:1.2102900074049494\n",
      "train loss:0.952131983856307\n",
      "train loss:0.9901113831092804\n",
      "train loss:1.031858953964785\n",
      "train loss:0.957303667291736\n",
      "train loss:1.2089290433344886\n",
      "train loss:1.1420044982255788\n",
      "train loss:1.008514306253658\n",
      "train loss:0.9786495424617062\n",
      "train loss:1.0361005587732306\n",
      "train loss:0.9802980456293539\n",
      "train loss:1.045816471842311\n",
      "train loss:1.114102271720838\n",
      "train loss:1.2212787871162216\n",
      "train loss:0.8371624128712816\n",
      "train loss:1.0687608371435535\n",
      "train loss:0.8921517133266745\n",
      "train loss:0.9738162750021909\n",
      "train loss:1.1203741602519564\n",
      "train loss:1.1723024492539607\n",
      "train loss:0.9720386670246076\n",
      "train loss:1.1718126431370495\n",
      "train loss:1.0239323635448039\n",
      "train loss:0.8730968448618246\n",
      "train loss:1.136646177473602\n",
      "train loss:0.8583764635568686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0589847412061923\n",
      "train loss:0.8557239874758086\n",
      "train loss:0.9394024557768063\n",
      "train loss:0.9057112827107009\n",
      "train loss:1.0650802436937539\n",
      "train loss:1.16095198566844\n",
      "train loss:0.8381932262944786\n",
      "train loss:0.8732107800408463\n",
      "train loss:1.1531011992430908\n",
      "train loss:0.9650918090511178\n",
      "train loss:0.9322269763323423\n",
      "train loss:1.023871128296014\n",
      "train loss:1.0152665386829083\n",
      "train loss:0.9911196224405414\n",
      "train loss:0.8876426193419489\n",
      "train loss:0.9721964607166567\n",
      "train loss:0.9541946566132588\n",
      "train loss:1.0677893645651038\n",
      "train loss:1.1234501424073275\n",
      "train loss:0.9285930631500718\n",
      "train loss:0.9674063792504414\n",
      "train loss:1.0437234103968844\n",
      "train loss:1.1307080165926036\n",
      "train loss:0.9209345749733062\n",
      "train loss:0.7298801251086526\n",
      "train loss:0.9918095391371782\n",
      "train loss:1.0149564253009995\n",
      "train loss:1.163397973580458\n",
      "train loss:0.8820845253825489\n",
      "train loss:1.026190572103494\n",
      "train loss:1.209856488142204\n",
      "train loss:1.0884135170468539\n",
      "train loss:0.8777559354464565\n",
      "train loss:0.9729247403225999\n",
      "train loss:1.012154227306165\n",
      "train loss:1.0046583588046305\n",
      "train loss:1.0095520877684152\n",
      "train loss:0.9719202050895512\n",
      "train loss:0.898805923362117\n",
      "train loss:1.0272256829472086\n",
      "train loss:1.1548198960966412\n",
      "train loss:0.9010042043467922\n",
      "train loss:0.9447780288594824\n",
      "train loss:0.9737751728595861\n",
      "train loss:0.956976553715725\n",
      "train loss:1.1369190764681745\n",
      "train loss:1.0819957813573817\n",
      "train loss:0.9442671676234089\n",
      "train loss:0.9650412818559468\n",
      "train loss:1.0934181834780627\n",
      "train loss:0.8935708646632957\n",
      "train loss:0.9246243717044155\n",
      "train loss:0.8867890834570165\n",
      "train loss:1.0641908554643984\n",
      "train loss:0.9569510440904111\n",
      "train loss:1.1213824178989267\n",
      "train loss:1.0052248832495563\n",
      "train loss:0.822844862297248\n",
      "train loss:1.0495534865338199\n",
      "train loss:0.9651572508995777\n",
      "train loss:0.99700764604652\n",
      "train loss:0.947915630124877\n",
      "train loss:1.0636430445485998\n",
      "train loss:0.9786669330601524\n",
      "train loss:0.9678348469226098\n",
      "train loss:0.9140447396105401\n",
      "train loss:1.084439395132413\n",
      "train loss:1.0252640365290666\n",
      "train loss:1.0506825960505737\n",
      "train loss:1.0583657230697099\n",
      "train loss:1.1615511859747563\n",
      "train loss:0.866713774239838\n",
      "train loss:0.9210459061575587\n",
      "train loss:1.2187490908608434\n",
      "train loss:0.8885200836283844\n",
      "train loss:1.1100772303360895\n",
      "train loss:0.8829515635993631\n",
      "train loss:0.9471465214000784\n",
      "train loss:1.1013193034798947\n",
      "train loss:1.1327851324601232\n",
      "train loss:1.0079919740245975\n",
      "train loss:0.97076537234891\n",
      "train loss:1.0403694302738626\n",
      "train loss:0.9100762902326764\n",
      "train loss:1.1130392901065418\n",
      "train loss:1.0775500435456897\n",
      "train loss:1.0729869128779734\n",
      "train loss:0.9263076013895126\n",
      "train loss:1.02597304240109\n",
      "train loss:1.100045885557398\n",
      "train loss:0.9618137667281735\n",
      "train loss:0.9822208990668777\n",
      "train loss:1.0228437772320567\n",
      "train loss:0.9465848780075052\n",
      "train loss:0.9684828133360429\n",
      "train loss:0.8542590046258505\n",
      "train loss:0.9547779698511646\n",
      "train loss:0.9751099802399268\n",
      "train loss:1.2412794363450221\n",
      "train loss:0.9907960636583142\n",
      "train loss:0.9426844994258748\n",
      "train loss:1.0058251379559924\n",
      "=== epoch:3, train acc:0.985, test acc:0.984 ===\n",
      "train loss:1.0843463205445227\n",
      "train loss:1.0242804704558384\n",
      "train loss:1.1145963639619492\n",
      "train loss:1.0067501276836428\n",
      "train loss:0.9609211383941969\n",
      "train loss:1.0721456991054974\n",
      "train loss:1.0002935999794886\n",
      "train loss:1.0420404887445502\n",
      "train loss:1.032058467396711\n",
      "train loss:0.8353806766652138\n",
      "train loss:0.9808835604766709\n",
      "train loss:0.9062233028687156\n",
      "train loss:1.0754618199697665\n",
      "train loss:1.07248898281831\n",
      "train loss:0.920323680213711\n",
      "train loss:1.0414047583084487\n",
      "train loss:0.9134589204553817\n",
      "train loss:1.061712189544884\n",
      "train loss:0.9495147592951233\n",
      "train loss:0.8850669983477728\n",
      "train loss:0.8818442446232713\n",
      "train loss:0.8998314766520362\n",
      "train loss:1.1568472369377905\n",
      "train loss:1.0501913150288769\n",
      "train loss:1.121739387639552\n",
      "train loss:1.0388298414378347\n",
      "train loss:0.961152287976774\n",
      "train loss:0.987168092340451\n",
      "train loss:0.757616659488554\n",
      "train loss:1.1132179078270263\n",
      "train loss:0.9140390151804769\n",
      "train loss:1.1366540454673881\n",
      "train loss:1.0475048918145702\n",
      "train loss:0.8652881773086598\n",
      "train loss:1.2369507314176378\n",
      "train loss:1.0514550117230552\n",
      "train loss:0.9700972005181525\n",
      "train loss:0.8811969380631165\n",
      "train loss:0.9307430916259828\n",
      "train loss:0.9232564729936509\n",
      "train loss:0.89990693779643\n",
      "train loss:0.9614154070044691\n",
      "train loss:1.0318928560154657\n",
      "train loss:0.8678663980837569\n",
      "train loss:0.910009220114327\n",
      "train loss:1.0604629081626957\n",
      "train loss:1.105420718891555\n",
      "train loss:1.0157726543407732\n",
      "train loss:0.9811167238502239\n",
      "train loss:0.947978991875965\n",
      "train loss:0.997435757029135\n",
      "train loss:1.0185311267851613\n",
      "train loss:0.9152325210169255\n",
      "train loss:1.092376685832674\n",
      "train loss:0.9882024408443246\n",
      "train loss:0.9519677250475052\n",
      "train loss:0.9304803313121051\n",
      "train loss:0.9234113503638781\n",
      "train loss:0.918436790172379\n",
      "train loss:1.1807305186014545\n",
      "train loss:1.1872401741105423\n",
      "train loss:0.9418350256062535\n",
      "train loss:1.0791585616429362\n",
      "train loss:0.9834988308592374\n",
      "train loss:0.9920902385824533\n",
      "train loss:0.9399071121423119\n",
      "train loss:0.9951682653422159\n",
      "train loss:0.8293034429947082\n",
      "train loss:0.9555803761932633\n",
      "train loss:0.9856867540609866\n",
      "train loss:0.9784618409641205\n",
      "train loss:1.0244344121545377\n",
      "train loss:1.0229410109152837\n",
      "train loss:0.9249890979845783\n",
      "train loss:1.0798541413570972\n",
      "train loss:1.2165827302493273\n",
      "train loss:0.9175552472708661\n",
      "train loss:1.045168768608342\n",
      "train loss:0.8631791392060562\n",
      "train loss:1.146668588936198\n",
      "train loss:0.9735924275565668\n",
      "train loss:1.1091066355566346\n",
      "train loss:0.9718241236312858\n",
      "train loss:0.865852524318109\n",
      "train loss:0.7823159154381152\n",
      "train loss:1.0639203394410628\n",
      "train loss:0.7481189691548812\n",
      "train loss:1.0214227616296125\n",
      "train loss:1.11716624691105\n",
      "train loss:1.0732405423387732\n",
      "train loss:1.0349045229014144\n",
      "train loss:0.9621559550042862\n",
      "train loss:0.9980427057912596\n",
      "train loss:1.1578123984441029\n",
      "train loss:0.8951210014705099\n",
      "train loss:0.9759285498737045\n",
      "train loss:1.0859562163941259\n",
      "train loss:1.1290440290721977\n",
      "train loss:0.9418497411040829\n",
      "train loss:0.9140523714722485\n",
      "train loss:0.9347408233585903\n",
      "train loss:0.914926813205969\n",
      "train loss:1.0746710392005394\n",
      "train loss:0.8911526349607353\n",
      "train loss:0.9520289486257415\n",
      "train loss:0.9172324285369109\n",
      "train loss:0.7993830071869789\n",
      "train loss:0.9813801094567848\n",
      "train loss:1.0171795383096147\n",
      "train loss:1.0659837893230506\n",
      "train loss:0.9412735250708966\n",
      "train loss:0.9515510968966105\n",
      "train loss:1.0111210798304575\n",
      "train loss:0.9086430301452353\n",
      "train loss:0.8136502793902415\n",
      "train loss:0.9672550524917244\n",
      "train loss:0.9598905564073593\n",
      "train loss:0.9864765155912459\n",
      "train loss:0.937052867419601\n",
      "train loss:1.1697949026234722\n",
      "train loss:0.9647680988691346\n",
      "train loss:0.968677953577814\n",
      "train loss:0.8116065862312922\n",
      "train loss:0.9753797146991559\n",
      "train loss:0.9801046866411803\n",
      "train loss:1.0953480948948895\n",
      "train loss:0.9563077476409432\n",
      "train loss:1.1928879510142547\n",
      "train loss:0.8866081158266725\n",
      "train loss:0.9894490272723716\n",
      "train loss:0.8761241700940867\n",
      "train loss:1.1112419707077041\n",
      "train loss:0.9617703921310223\n",
      "train loss:0.985589266054825\n",
      "train loss:0.9853112936470084\n",
      "train loss:1.1360892441826613\n",
      "train loss:1.0874336383656402\n",
      "train loss:0.8687436082145368\n",
      "train loss:1.0591291946775727\n",
      "train loss:1.1452519408647277\n",
      "train loss:1.0015813460562724\n",
      "train loss:1.0017591365275655\n",
      "train loss:0.9258658947127857\n",
      "train loss:0.9278127449839986\n",
      "train loss:1.0555596747239304\n",
      "train loss:0.8827723374302059\n",
      "train loss:0.9657403785781767\n",
      "train loss:0.9556138612364994\n",
      "train loss:0.7832729549527034\n",
      "train loss:1.0746609637560116\n",
      "train loss:1.0412230536204088\n",
      "train loss:1.1384493090804517\n",
      "train loss:1.0597318322929405\n",
      "train loss:1.1724300373178433\n",
      "train loss:0.9139303829957437\n",
      "train loss:1.0639637452635906\n",
      "train loss:0.9300983853854503\n",
      "train loss:1.0332124584149363\n",
      "train loss:1.1087850161676456\n",
      "train loss:0.9668145683263866\n",
      "train loss:1.3239322572428776\n",
      "train loss:1.004565993400451\n",
      "train loss:1.021860220737767\n",
      "train loss:1.0399515851290622\n",
      "train loss:0.9827716720928739\n",
      "train loss:1.0582598602505484\n",
      "train loss:0.8283649990904451\n",
      "train loss:1.091762811840623\n",
      "train loss:0.9614674136672247\n",
      "train loss:1.0312377757498883\n",
      "train loss:1.1392375670550818\n",
      "train loss:1.10935924658529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.105970480793773\n",
      "train loss:1.0849136276843332\n",
      "train loss:0.9059116536560129\n",
      "train loss:0.7731454864318126\n",
      "train loss:1.0755160258775753\n",
      "train loss:0.8479185294049078\n",
      "train loss:0.9542808212310787\n",
      "train loss:0.8902600142518499\n",
      "train loss:1.0356869839734675\n",
      "train loss:0.8644510184795624\n",
      "train loss:1.090942209267686\n",
      "train loss:0.7851722725117649\n",
      "train loss:0.9655209620049399\n",
      "train loss:1.0322123367957674\n",
      "train loss:0.9448761853692629\n",
      "train loss:1.1062539300155196\n",
      "train loss:0.7809333274360816\n",
      "train loss:1.0349525197188627\n",
      "train loss:0.9594453122366332\n",
      "train loss:0.8663909415207044\n",
      "train loss:0.8410938069735129\n",
      "train loss:0.9064493752574259\n",
      "train loss:0.9827090542125839\n",
      "train loss:1.0819333909825437\n",
      "train loss:0.9566213763423539\n",
      "train loss:1.0029357055321664\n",
      "train loss:0.93843818065306\n",
      "train loss:1.011698833386792\n",
      "train loss:1.0222666289715001\n",
      "train loss:0.9154843582407531\n",
      "train loss:0.8702681611863413\n",
      "train loss:1.0355771261542224\n",
      "train loss:0.9058756858854422\n",
      "train loss:1.154786451977038\n",
      "train loss:0.9719832264684478\n",
      "train loss:1.028420235899711\n",
      "train loss:0.8891015430909366\n",
      "train loss:0.9133484280931025\n",
      "train loss:1.1292188095938815\n",
      "train loss:1.0567317668490213\n",
      "train loss:1.09171878348003\n",
      "train loss:1.0002503434134409\n",
      "train loss:1.0652456516266926\n",
      "train loss:0.756902562154698\n",
      "train loss:0.9364064357268873\n",
      "train loss:0.9990065433165232\n",
      "train loss:0.8951360457433865\n",
      "train loss:1.073280231940032\n",
      "train loss:1.0124731749769171\n",
      "train loss:0.8140793857274319\n",
      "train loss:0.8907096864788453\n",
      "train loss:0.980389777920057\n",
      "train loss:0.8559986717728093\n",
      "train loss:1.0267450970161969\n",
      "train loss:1.0614114435009296\n",
      "train loss:0.9238906602561202\n",
      "train loss:1.0173561859830011\n",
      "train loss:0.9608833812451032\n",
      "train loss:0.8526679441188396\n",
      "train loss:0.9144866122670754\n",
      "train loss:1.0677636126093375\n",
      "train loss:1.1392157346398222\n",
      "train loss:0.9131054262380526\n",
      "train loss:0.7605862933442978\n",
      "train loss:1.189106743228626\n",
      "train loss:0.950725721093666\n",
      "train loss:0.9902756132044611\n",
      "train loss:0.9706485902175074\n",
      "train loss:1.0020575404596759\n",
      "train loss:1.0736466805365257\n",
      "train loss:0.9712008787131592\n",
      "train loss:0.9815422683769222\n",
      "train loss:0.9087829553807809\n",
      "train loss:0.8642640845724188\n",
      "train loss:0.8581653964005675\n",
      "train loss:0.9764886167574559\n",
      "train loss:1.0869312110125886\n",
      "train loss:0.8898859199136031\n",
      "train loss:0.9434720417798189\n",
      "train loss:1.113329826534571\n",
      "train loss:0.92085911042361\n",
      "train loss:0.7872798074622839\n",
      "train loss:0.9121922281948928\n",
      "train loss:0.9222959507971149\n",
      "train loss:1.164406357396331\n",
      "train loss:1.0267224505026977\n",
      "train loss:0.8983318140732324\n",
      "train loss:1.0968331493191688\n",
      "train loss:1.027142484846851\n",
      "train loss:0.8279216349920552\n",
      "train loss:1.0548924199172784\n",
      "train loss:1.0540405723020096\n",
      "train loss:0.8256711844245075\n",
      "train loss:1.1200651844454403\n",
      "train loss:0.9319436303862276\n",
      "train loss:0.9574765930042628\n",
      "train loss:0.8835630326920839\n",
      "train loss:0.9333396370683009\n",
      "train loss:0.8883183450856549\n",
      "train loss:0.962065062149314\n",
      "train loss:1.015883725840553\n",
      "train loss:0.8210646654787966\n",
      "train loss:0.9964654072136229\n",
      "train loss:0.9928632961534416\n",
      "train loss:0.9856546174433308\n",
      "train loss:0.9122754950366935\n",
      "train loss:1.2393406555233626\n",
      "train loss:1.0283802218805291\n",
      "train loss:1.0413676587174918\n",
      "train loss:1.0438584803557556\n",
      "train loss:1.03516319439546\n",
      "train loss:0.932529968801889\n",
      "train loss:0.9927448647197962\n",
      "train loss:0.8774346666475418\n",
      "train loss:1.1455782763849391\n",
      "train loss:0.8612603931724536\n",
      "train loss:0.9753766101837711\n",
      "train loss:0.8970113053113274\n",
      "train loss:0.9385247460490607\n",
      "train loss:0.8454531270274852\n",
      "train loss:1.0028585826561298\n",
      "train loss:0.8816442286901612\n",
      "train loss:0.9474517418678359\n",
      "train loss:1.0079559110134115\n",
      "train loss:1.0947736611834404\n",
      "train loss:1.0610149041011043\n",
      "train loss:1.0383400944663757\n",
      "train loss:0.998146326766724\n",
      "train loss:0.8873441103449544\n",
      "train loss:0.9449351287659203\n",
      "train loss:1.121474197849482\n",
      "train loss:1.036658793130294\n",
      "train loss:1.1402286951996359\n",
      "train loss:0.8627635503442319\n",
      "train loss:1.0570057855178987\n",
      "train loss:0.9412614844378757\n",
      "train loss:0.897207170045453\n",
      "train loss:1.2145502903794227\n",
      "train loss:0.9561614193052634\n",
      "train loss:1.0073066631222032\n",
      "train loss:0.9402960742149304\n",
      "train loss:1.2044236584769503\n",
      "train loss:1.0897754400615849\n",
      "train loss:1.0308442913422793\n",
      "train loss:1.034978046822269\n",
      "train loss:0.894251308016582\n",
      "train loss:0.946709641774222\n",
      "train loss:1.0951380678769507\n",
      "train loss:0.9926014445076504\n",
      "train loss:0.9221423630251864\n",
      "train loss:0.9719045670179444\n",
      "train loss:0.8237343645176477\n",
      "train loss:0.9536439064832345\n",
      "train loss:0.6921395745922574\n",
      "train loss:0.9792105990082728\n",
      "train loss:0.8994241977242983\n",
      "train loss:0.8502061559549142\n",
      "train loss:0.9680069416307795\n",
      "train loss:0.8849432029479855\n",
      "train loss:0.9216003909152144\n",
      "train loss:1.0498209165338805\n",
      "train loss:0.945031813731163\n",
      "train loss:0.851829459500816\n",
      "train loss:1.002151444811606\n",
      "train loss:0.8400036223983284\n",
      "train loss:0.944489071303342\n",
      "train loss:1.0067407200749536\n",
      "train loss:0.9437781442530268\n",
      "train loss:1.0329738125828143\n",
      "train loss:0.8972230919307749\n",
      "train loss:0.9668130150006227\n",
      "train loss:1.0750432109974586\n",
      "train loss:0.9515944178277429\n",
      "train loss:0.8569914608235161\n",
      "train loss:1.0838231858144098\n",
      "train loss:0.9414010438468461\n",
      "train loss:1.0100618715154905\n",
      "train loss:0.9917219246040335\n",
      "train loss:0.9787545046981718\n",
      "train loss:0.8194199865031808\n",
      "train loss:0.8655209373811755\n",
      "train loss:1.0403895110063406\n",
      "train loss:0.8893387935620418\n",
      "train loss:0.9096070432149571\n",
      "train loss:0.9387954797372697\n",
      "train loss:0.8897642283152801\n",
      "train loss:0.8739695317997243\n",
      "train loss:0.9892453383392934\n",
      "train loss:0.9044116716073212\n",
      "train loss:0.8329869150892955\n",
      "train loss:0.9711677753344223\n",
      "train loss:0.9567318206482841\n",
      "train loss:0.9876455536548332\n",
      "train loss:0.8918049906443888\n",
      "train loss:0.902268966867133\n",
      "train loss:0.9131711676447586\n",
      "train loss:0.9062338260714979\n",
      "train loss:0.9979453709509023\n",
      "train loss:0.9292065557157261\n",
      "train loss:1.0308526228413841\n",
      "train loss:0.850868661305737\n",
      "train loss:0.9238937154063029\n",
      "train loss:1.1505288855170581\n",
      "train loss:1.0210395148076985\n",
      "train loss:0.9957957655034432\n",
      "train loss:0.9995733166743302\n",
      "train loss:1.1636890590246087\n",
      "train loss:1.0170650898361113\n",
      "train loss:1.0629370611975346\n",
      "train loss:0.9081233854994353\n",
      "train loss:1.1177047729534053\n",
      "train loss:1.062447594016095\n",
      "train loss:1.074498406521474\n",
      "train loss:0.8921789946405478\n",
      "train loss:0.9423465506836667\n",
      "train loss:1.0411335065921232\n",
      "train loss:0.8793297557820352\n",
      "train loss:0.8923391444902856\n",
      "train loss:0.9301143551713877\n",
      "train loss:0.967878361455397\n",
      "train loss:0.9270811958303584\n",
      "train loss:0.921602910512968\n",
      "train loss:1.0043665461264955\n",
      "train loss:0.7772570223951675\n",
      "train loss:1.1636434665251663\n",
      "train loss:0.8281984044761097\n",
      "train loss:0.8219117303517358\n",
      "train loss:1.0260663256907532\n",
      "train loss:1.0948214258474998\n",
      "train loss:0.8838365016348692\n",
      "train loss:0.9766450263862716\n",
      "train loss:0.9545834221740881\n",
      "train loss:1.0612272285431514\n",
      "train loss:1.1146812142143439\n",
      "train loss:1.0239381265564296\n",
      "train loss:0.9114449871216733\n",
      "train loss:0.9627348457889764\n",
      "train loss:0.979598476862037\n",
      "train loss:1.1394219522417084\n",
      "train loss:0.9605626350975122\n",
      "train loss:0.8715159278796172\n",
      "train loss:0.9765462914781555\n",
      "train loss:0.9965051906576093\n",
      "train loss:0.867778610575612\n",
      "train loss:0.9061789559297699\n",
      "train loss:0.8818461273825978\n",
      "train loss:0.818932179669827\n",
      "train loss:0.90805736923928\n",
      "train loss:1.0273058356939426\n",
      "train loss:1.053705983438971\n",
      "train loss:0.740034643222791\n",
      "train loss:0.965188577856686\n",
      "train loss:0.7685126281444574\n",
      "train loss:1.1169642225964522\n",
      "train loss:0.922420949496905\n",
      "train loss:1.0245401311494913\n",
      "train loss:0.7749942821853817\n",
      "train loss:1.0336177490181138\n",
      "train loss:1.0703338993657265\n",
      "train loss:1.0694527125396156\n",
      "train loss:0.9734796975573111\n",
      "train loss:0.8577538282834151\n",
      "train loss:0.9174386878536359\n",
      "train loss:0.9405184265670817\n",
      "train loss:0.8933614022382763\n",
      "train loss:1.1120903261962538\n",
      "train loss:1.0267256609212985\n",
      "train loss:1.0992077823471735\n",
      "train loss:0.9981108941367524\n",
      "train loss:0.8620338548897201\n",
      "train loss:1.0174974050477292\n",
      "train loss:1.0449404995747122\n",
      "train loss:0.9099098187063608\n",
      "train loss:1.0526255394933677\n",
      "train loss:0.8441933464811742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9234609352069814\n",
      "train loss:0.948268549353117\n",
      "train loss:0.9767587187913422\n",
      "train loss:0.9582809557672592\n",
      "train loss:0.812007781665885\n",
      "train loss:1.0431772721125199\n",
      "train loss:0.8836768981575662\n",
      "train loss:0.9861305940586087\n",
      "train loss:0.9065080763191793\n",
      "train loss:1.2156191517764454\n",
      "train loss:0.8010205436408977\n",
      "train loss:1.0490619309380325\n",
      "train loss:0.9654120107916379\n",
      "train loss:1.0463608501239245\n",
      "train loss:1.0829683610398662\n",
      "train loss:0.8513524420058232\n",
      "train loss:0.9601997798999787\n",
      "train loss:0.9397150685973219\n",
      "train loss:1.01888933833875\n",
      "train loss:0.8036057201252693\n",
      "train loss:1.062452455635691\n",
      "train loss:0.9059394742075014\n",
      "train loss:0.8935903366025891\n",
      "train loss:1.1179058375924666\n",
      "train loss:0.9129815562059052\n",
      "train loss:0.8772267407012017\n",
      "train loss:0.8892554351038392\n",
      "train loss:0.8838825733202019\n",
      "train loss:0.9480463025528101\n",
      "train loss:0.9632653415977447\n",
      "train loss:1.0619981824710192\n",
      "train loss:0.8013425470069965\n",
      "train loss:1.004775933858747\n",
      "train loss:0.8420907428128832\n",
      "train loss:1.0481568645481039\n",
      "train loss:0.8905705050822746\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7023c61d5697>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 保存参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\deep_convnet.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\deep_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\deep_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mcol_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\深度学习入门基于Python的理论与实现\\Chapter08\\common\\util.py\u001b[0m in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mx_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mcol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_h\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mout_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ee194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

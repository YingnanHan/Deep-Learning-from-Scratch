{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d30f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29938662680664\n",
      "=== epoch:1, train acc:0.281, test acc:0.304 ===\n",
      "train loss:2.2984897298108646\n",
      "train loss:2.2930967987058297\n",
      "train loss:2.2829144231593093\n",
      "train loss:2.2810095401829047\n",
      "train loss:2.261727755449575\n",
      "train loss:2.2604498355948848\n",
      "train loss:2.2430974686073055\n",
      "train loss:2.2050343851518956\n",
      "train loss:2.2214594861983015\n",
      "train loss:2.132157279469612\n",
      "train loss:2.1604313361385947\n",
      "train loss:2.1109094346588937\n",
      "train loss:2.034437506825552\n",
      "train loss:2.0035147463807754\n",
      "train loss:1.9332635987658562\n",
      "train loss:1.9405813028617347\n",
      "train loss:1.838847482576217\n",
      "train loss:1.7553568321407473\n",
      "train loss:1.752695949343952\n",
      "train loss:1.6117159667784058\n",
      "train loss:1.4822533793857462\n",
      "train loss:1.4241585070887908\n",
      "train loss:1.228260844622931\n",
      "train loss:1.3426173476192123\n",
      "train loss:1.1020300467044235\n",
      "train loss:1.1145959238556447\n",
      "train loss:1.0697990570127116\n",
      "train loss:1.0234069649061885\n",
      "train loss:0.9362056391229997\n",
      "train loss:0.7995060758844122\n",
      "train loss:0.8126512768003827\n",
      "train loss:0.8503795541626598\n",
      "train loss:0.7326260960956786\n",
      "train loss:0.7825065889117164\n",
      "train loss:0.8757350833468419\n",
      "train loss:0.661719621508936\n",
      "train loss:0.5632339806546062\n",
      "train loss:0.4903889376206306\n",
      "train loss:0.6320157228986121\n",
      "train loss:0.6913033053554232\n",
      "train loss:0.6236198359195175\n",
      "train loss:0.5300360460193762\n",
      "train loss:0.9987720812647484\n",
      "train loss:0.5838429255145583\n",
      "train loss:0.5554582207103208\n",
      "train loss:0.5193256287047038\n",
      "train loss:0.5902172532660982\n",
      "train loss:0.534045143504414\n",
      "train loss:0.564411496617759\n",
      "train loss:0.5439182006719897\n",
      "=== epoch:2, train acc:0.798, test acc:0.788 ===\n",
      "train loss:0.6080387745336586\n",
      "train loss:0.4981976064302656\n",
      "train loss:0.410591004610581\n",
      "train loss:0.3862015048848241\n",
      "train loss:0.4171846287978944\n",
      "train loss:0.4483184993941876\n",
      "train loss:0.39219826016963344\n",
      "train loss:0.4196445907036697\n",
      "train loss:0.5670094293358934\n",
      "train loss:0.39044304857457285\n",
      "train loss:0.4677969945603207\n",
      "train loss:0.39253234915080093\n",
      "train loss:0.3116396117439323\n",
      "train loss:0.3293154865088709\n",
      "train loss:0.46783805259001127\n",
      "train loss:0.5023366173458078\n",
      "train loss:0.3135126425036087\n",
      "train loss:0.38187907655113607\n",
      "train loss:0.46658516438539754\n",
      "train loss:0.3931524120530942\n",
      "train loss:0.4457222489868667\n",
      "train loss:0.37804031899192436\n",
      "train loss:0.4343722483652133\n",
      "train loss:0.35337562277217516\n",
      "train loss:0.27905130763777275\n",
      "train loss:0.3983797397020844\n",
      "train loss:0.49439806002641906\n",
      "train loss:0.5941192219111908\n",
      "train loss:0.3752686848242017\n",
      "train loss:0.3161682183053134\n",
      "train loss:0.4150449749657338\n",
      "train loss:0.408518324104387\n",
      "train loss:0.4439517784110512\n",
      "train loss:0.4072982339054183\n",
      "train loss:0.22205056830848316\n",
      "train loss:0.2648831087867067\n",
      "train loss:0.4110675803967138\n",
      "train loss:0.42285352782072927\n",
      "train loss:0.49285821939567803\n",
      "train loss:0.3279760781039782\n",
      "train loss:0.2435083468238244\n",
      "train loss:0.28317322553401647\n",
      "train loss:0.3505138228359955\n",
      "train loss:0.36056391205480715\n",
      "train loss:0.432084144433011\n",
      "train loss:0.37461796253665347\n",
      "train loss:0.29335617278323206\n",
      "train loss:0.37348547310590574\n",
      "train loss:0.18210050060006389\n",
      "train loss:0.27871461255341945\n",
      "=== epoch:3, train acc:0.862, test acc:0.852 ===\n",
      "train loss:0.28245852719555575\n",
      "train loss:0.4563489763224451\n",
      "train loss:0.39308390103802227\n",
      "train loss:0.4273479509524081\n",
      "train loss:0.4223004304516396\n",
      "train loss:0.2675425487274306\n",
      "train loss:0.42355036229041754\n",
      "train loss:0.3047219480782357\n",
      "train loss:0.3178162872617956\n",
      "train loss:0.24444034280579527\n",
      "train loss:0.422392885472013\n",
      "train loss:0.34044795428709307\n",
      "train loss:0.2490168540525728\n",
      "train loss:0.32910119201465593\n",
      "train loss:0.3070387775732208\n",
      "train loss:0.4156395963628822\n",
      "train loss:0.329232937505583\n",
      "train loss:0.3294269765140211\n",
      "train loss:0.42206854242049036\n",
      "train loss:0.33946343477058394\n",
      "train loss:0.2261362076638584\n",
      "train loss:0.2408171892423891\n",
      "train loss:0.2265457412866924\n",
      "train loss:0.41411720038301475\n",
      "train loss:0.33429350990794193\n",
      "train loss:0.5518393784284817\n",
      "train loss:0.26431623833809637\n",
      "train loss:0.37805488088142525\n",
      "train loss:0.5029214045940513\n",
      "train loss:0.49750080734321067\n",
      "train loss:0.40159509066132953\n",
      "train loss:0.31541237682052786\n",
      "train loss:0.21512955365559847\n",
      "train loss:0.28229329390971314\n",
      "train loss:0.29752835010578993\n",
      "train loss:0.2036907417313366\n",
      "train loss:0.22958131485252958\n",
      "train loss:0.22863703617540868\n",
      "train loss:0.5127712446827537\n",
      "train loss:0.1812715683923153\n",
      "train loss:0.4085357227531932\n",
      "train loss:0.2146922974670116\n",
      "train loss:0.2949736013205384\n",
      "train loss:0.34287164106633644\n",
      "train loss:0.33892936946392405\n",
      "train loss:0.3084713131164171\n",
      "train loss:0.3302682667227937\n",
      "train loss:0.15802763062536512\n",
      "train loss:0.35342918975796656\n",
      "train loss:0.33692310333981695\n",
      "=== epoch:4, train acc:0.891, test acc:0.87 ===\n",
      "train loss:0.1980700261339394\n",
      "train loss:0.3207692182687621\n",
      "train loss:0.2950925747358511\n",
      "train loss:0.26245769197708074\n",
      "train loss:0.32049231379170545\n",
      "train loss:0.22864067230527485\n",
      "train loss:0.330591366916753\n",
      "train loss:0.2208186337717429\n",
      "train loss:0.3122159328355672\n",
      "train loss:0.1573055343412126\n",
      "train loss:0.13199389258559607\n",
      "train loss:0.25941103588485603\n",
      "train loss:0.2689760315323302\n",
      "train loss:0.26696534912777836\n",
      "train loss:0.28024287914447327\n",
      "train loss:0.30841301077370203\n",
      "train loss:0.18930171411128122\n",
      "train loss:0.29879034916054265\n",
      "train loss:0.32657720380885535\n",
      "train loss:0.3194984488080889\n",
      "train loss:0.2719598913142759\n",
      "train loss:0.40545747709603447\n",
      "train loss:0.19105655937769442\n",
      "train loss:0.37165944541165735\n",
      "train loss:0.38111822577252896\n",
      "train loss:0.25695342024276313\n",
      "train loss:0.18942100471085171\n",
      "train loss:0.231366295048637\n",
      "train loss:0.1372520710225931\n",
      "train loss:0.34870693293431065\n",
      "train loss:0.35399038440937486\n",
      "train loss:0.2127499138244267\n",
      "train loss:0.17736200416081402\n",
      "train loss:0.26290204077943086\n",
      "train loss:0.3460887737486942\n",
      "train loss:0.20419805473944322\n",
      "train loss:0.14268772181900566\n",
      "train loss:0.21728581957991427\n",
      "train loss:0.30691477403027306\n",
      "train loss:0.10707681626724791\n",
      "train loss:0.22419134133557755\n",
      "train loss:0.33841962158951716\n",
      "train loss:0.20974190654683553\n",
      "train loss:0.3152546678850896\n",
      "train loss:0.14053506149520886\n",
      "train loss:0.3622805445405137\n",
      "train loss:0.2023429666882495\n",
      "train loss:0.30933791146049444\n",
      "train loss:0.2362652495621694\n",
      "train loss:0.15604993130290015\n",
      "=== epoch:5, train acc:0.903, test acc:0.887 ===\n",
      "train loss:0.3274174742971757\n",
      "train loss:0.20862163778582996\n",
      "train loss:0.35051143638974147\n",
      "train loss:0.1974548228159385\n",
      "train loss:0.24838866389992048\n",
      "train loss:0.22133847544023078\n",
      "train loss:0.2659905449674916\n",
      "train loss:0.2950103224063285\n",
      "train loss:0.4530348677438969\n",
      "train loss:0.1817880299589762\n",
      "train loss:0.16675881510067878\n",
      "train loss:0.1767170325365262\n",
      "train loss:0.22044794062106216\n",
      "train loss:0.14393693116812498\n",
      "train loss:0.30573862840753807\n",
      "train loss:0.24761844578221687\n",
      "train loss:0.13210202508070518\n",
      "train loss:0.1598915962811445\n",
      "train loss:0.16849235792152314\n",
      "train loss:0.24308808298734352\n",
      "train loss:0.2873127980819191\n",
      "train loss:0.21777114526049587\n",
      "train loss:0.1467671098955007\n",
      "train loss:0.2601634005480089\n",
      "train loss:0.16396407269471336\n",
      "train loss:0.2672284746711744\n",
      "train loss:0.24847765911428513\n",
      "train loss:0.2892343457116866\n",
      "train loss:0.26825521299533656\n",
      "train loss:0.2149825865561424\n",
      "train loss:0.21105640126080324\n",
      "train loss:0.31147948013217946\n",
      "train loss:0.14732076727955362\n",
      "train loss:0.19109680946372912\n",
      "train loss:0.20805139862404645\n",
      "train loss:0.3233206748463151\n",
      "train loss:0.23384290243407183\n",
      "train loss:0.3123918149453582\n",
      "train loss:0.20441431333578916\n",
      "train loss:0.2058635727927418\n",
      "train loss:0.17520432287650656\n",
      "train loss:0.23007043702614863\n",
      "train loss:0.10785004360725317\n",
      "train loss:0.301576365126231\n",
      "train loss:0.13768727809829115\n",
      "train loss:0.11713186464452909\n",
      "train loss:0.21397336323644298\n",
      "train loss:0.11533269452218374\n",
      "train loss:0.22674722174369533\n",
      "train loss:0.20244753594422768\n",
      "=== epoch:6, train acc:0.906, test acc:0.895 ===\n",
      "train loss:0.1918445661402606\n",
      "train loss:0.15158077902525588\n",
      "train loss:0.19359561881609777\n",
      "train loss:0.20739000888258213\n",
      "train loss:0.26285884785977526\n",
      "train loss:0.06588178463585057\n",
      "train loss:0.17500508036558104\n",
      "train loss:0.1756120590803439\n",
      "train loss:0.12474734814881785\n",
      "train loss:0.15775556532651683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08403932821668836\n",
      "train loss:0.1274014550760532\n",
      "train loss:0.1726601630279533\n",
      "train loss:0.18295896776623485\n",
      "train loss:0.1933431379315592\n",
      "train loss:0.1665546668951348\n",
      "train loss:0.18863149332992835\n",
      "train loss:0.264500858013481\n",
      "train loss:0.2445994196738677\n",
      "train loss:0.20125944243998325\n",
      "train loss:0.18671648313229533\n",
      "train loss:0.21862487782468462\n",
      "train loss:0.231984392315376\n",
      "train loss:0.23966039705753212\n",
      "train loss:0.08630614513252728\n",
      "train loss:0.2113322852394113\n",
      "train loss:0.11053171269462972\n",
      "train loss:0.1716078692516082\n",
      "train loss:0.40127746317814683\n",
      "train loss:0.17683238383927255\n",
      "train loss:0.15019988403526166\n",
      "train loss:0.20086212115428403\n",
      "train loss:0.30953960256356916\n",
      "train loss:0.17709807634637895\n",
      "train loss:0.30635005128085596\n",
      "train loss:0.20817964251210483\n",
      "train loss:0.27679487897771776\n",
      "train loss:0.18445027722700452\n",
      "train loss:0.13235405304990996\n",
      "train loss:0.21565226086720052\n",
      "train loss:0.18246139434732803\n",
      "train loss:0.24446076281187193\n",
      "train loss:0.14058000891107422\n",
      "train loss:0.1658346103308131\n",
      "train loss:0.31502277896879344\n",
      "train loss:0.2355083736691385\n",
      "train loss:0.1353513384531597\n",
      "train loss:0.11579238494070442\n",
      "train loss:0.1686283282794737\n",
      "train loss:0.3350369531469263\n",
      "=== epoch:7, train acc:0.926, test acc:0.913 ===\n",
      "train loss:0.10404237353124568\n",
      "train loss:0.20220456852042024\n",
      "train loss:0.24308532492324197\n",
      "train loss:0.24513119097784727\n",
      "train loss:0.2579778892858143\n",
      "train loss:0.2399103605630156\n",
      "train loss:0.20296150532435006\n",
      "train loss:0.2044755598675695\n",
      "train loss:0.1749601562002315\n",
      "train loss:0.260327145079102\n",
      "train loss:0.13732453319378046\n",
      "train loss:0.2098045532058775\n",
      "train loss:0.20928216450624584\n",
      "train loss:0.2326072249892282\n",
      "train loss:0.16140546216960983\n",
      "train loss:0.2365640903731834\n",
      "train loss:0.1638889666471382\n",
      "train loss:0.1506628761007765\n",
      "train loss:0.22992864142975647\n",
      "train loss:0.24788262821389093\n",
      "train loss:0.08004428976810217\n",
      "train loss:0.2173742048392728\n",
      "train loss:0.16530960910849474\n",
      "train loss:0.1561423830418085\n",
      "train loss:0.10269931371834846\n",
      "train loss:0.22218716359550186\n",
      "train loss:0.19736727887492334\n",
      "train loss:0.1144016904425515\n",
      "train loss:0.10532376360645923\n",
      "train loss:0.1716883925095755\n",
      "train loss:0.12200797913768174\n",
      "train loss:0.2552976190565446\n",
      "train loss:0.191049313345416\n",
      "train loss:0.18503705558353975\n",
      "train loss:0.1773282324404542\n",
      "train loss:0.22874662355732953\n",
      "train loss:0.13897346449580644\n",
      "train loss:0.11695009041800981\n",
      "train loss:0.1944180362030454\n",
      "train loss:0.15713885685441217\n",
      "train loss:0.14355193701034882\n",
      "train loss:0.14655671832710537\n",
      "train loss:0.12052777918526479\n",
      "train loss:0.1892746485222579\n",
      "train loss:0.11712679256470905\n",
      "train loss:0.17933345111267535\n",
      "train loss:0.13800865580725077\n",
      "train loss:0.1278700481271106\n",
      "train loss:0.06456961645478271\n",
      "train loss:0.16099436242086995\n",
      "=== epoch:8, train acc:0.94, test acc:0.919 ===\n",
      "train loss:0.12072396699657363\n",
      "train loss:0.14839645677045155\n",
      "train loss:0.24325023742364607\n",
      "train loss:0.19480172118013658\n",
      "train loss:0.16584103444195125\n",
      "train loss:0.13481931602189706\n",
      "train loss:0.12526004324207698\n",
      "train loss:0.16071003012891896\n",
      "train loss:0.12345055348050642\n",
      "train loss:0.20389573264313576\n",
      "train loss:0.1730841974542473\n",
      "train loss:0.19614123145350387\n",
      "train loss:0.2799035813911602\n",
      "train loss:0.13698065165663048\n",
      "train loss:0.08389743764482517\n",
      "train loss:0.14692039127877762\n",
      "train loss:0.13322055822499645\n",
      "train loss:0.15552497602740517\n",
      "train loss:0.14099464858608585\n",
      "train loss:0.13899664399870384\n",
      "train loss:0.08940270976438516\n",
      "train loss:0.10576866275194442\n",
      "train loss:0.17830250248288024\n",
      "train loss:0.1283314887698395\n",
      "train loss:0.12804732229094865\n",
      "train loss:0.1716425409053783\n",
      "train loss:0.1203096782699377\n",
      "train loss:0.0551449884859565\n",
      "train loss:0.15796698082011976\n",
      "train loss:0.19555540697630136\n",
      "train loss:0.1712497267672185\n",
      "train loss:0.21497370655217035\n",
      "train loss:0.1279060288596896\n",
      "train loss:0.20992011975710445\n",
      "train loss:0.08830625426370756\n",
      "train loss:0.18539939502205122\n",
      "train loss:0.20413402415167547\n",
      "train loss:0.08851724960330323\n",
      "train loss:0.13658263026158457\n",
      "train loss:0.22667728767554685\n",
      "train loss:0.07515851119260682\n",
      "train loss:0.13514143097412723\n",
      "train loss:0.16033234419681708\n",
      "train loss:0.13805059862668226\n",
      "train loss:0.15672468497435538\n",
      "train loss:0.1066218145687252\n",
      "train loss:0.21141391432555343\n",
      "train loss:0.10330454151849677\n",
      "train loss:0.09668009668879755\n",
      "train loss:0.11867230681036622\n",
      "=== epoch:9, train acc:0.954, test acc:0.933 ===\n",
      "train loss:0.04366084244506773\n",
      "train loss:0.1905453843367975\n",
      "train loss:0.13720125171589057\n",
      "train loss:0.08108625800797237\n",
      "train loss:0.11954239146520156\n",
      "train loss:0.1128855289761572\n",
      "train loss:0.0971485919243127\n",
      "train loss:0.0686345625306875\n",
      "train loss:0.13582547835649983\n",
      "train loss:0.05678390456736519\n",
      "train loss:0.14134412768995047\n",
      "train loss:0.1555538491003374\n",
      "train loss:0.06723403708652045\n",
      "train loss:0.13802966716101783\n",
      "train loss:0.10841517931497431\n",
      "train loss:0.06308738166162321\n",
      "train loss:0.11395351366412154\n",
      "train loss:0.17614377340339332\n",
      "train loss:0.15196759289265047\n",
      "train loss:0.10474123925068199\n",
      "train loss:0.17314482116650445\n",
      "train loss:0.17616559717346866\n",
      "train loss:0.0787795237999854\n",
      "train loss:0.10156544920577491\n",
      "train loss:0.07289786828231423\n",
      "train loss:0.05373083454861389\n",
      "train loss:0.14120467591396882\n",
      "train loss:0.20153600396502516\n",
      "train loss:0.08157191149484437\n",
      "train loss:0.1003405444229069\n",
      "train loss:0.11001423944596109\n",
      "train loss:0.08909839334402721\n",
      "train loss:0.20393676955270518\n",
      "train loss:0.15833964554862398\n",
      "train loss:0.08906361415714797\n",
      "train loss:0.19002238054962317\n",
      "train loss:0.08221607602997033\n",
      "train loss:0.12713042693528664\n",
      "train loss:0.10873490319295717\n",
      "train loss:0.11828509405155321\n",
      "train loss:0.1932183018317689\n",
      "train loss:0.0958964950358746\n",
      "train loss:0.09032108998565622\n",
      "train loss:0.05823397401681205\n",
      "train loss:0.25898675956857065\n",
      "train loss:0.18672074500131505\n",
      "train loss:0.10457584165202632\n",
      "train loss:0.137538047646833\n",
      "train loss:0.1265099265774583\n",
      "train loss:0.16332178899785701\n",
      "=== epoch:10, train acc:0.951, test acc:0.939 ===\n",
      "train loss:0.12680728255269766\n",
      "train loss:0.16511111702738585\n",
      "train loss:0.043925869210793354\n",
      "train loss:0.11082295891585807\n",
      "train loss:0.07495695913778906\n",
      "train loss:0.06498112892288309\n",
      "train loss:0.19703299682299225\n",
      "train loss:0.06958764219551988\n",
      "train loss:0.17093168594382607\n",
      "train loss:0.05641730145625459\n",
      "train loss:0.1321269470947067\n",
      "train loss:0.17177939774655468\n",
      "train loss:0.1401452348120288\n",
      "train loss:0.09663632939638646\n",
      "train loss:0.07981827605498541\n",
      "train loss:0.08582995562562827\n",
      "train loss:0.16947396338124418\n",
      "train loss:0.11412575396232928\n",
      "train loss:0.11326601892610423\n",
      "train loss:0.09180697866971382\n",
      "train loss:0.06835517942432871\n",
      "train loss:0.14032731383439695\n",
      "train loss:0.11357298625913975\n",
      "train loss:0.16868123195550364\n",
      "train loss:0.10004645337196438\n",
      "train loss:0.09710151892976786\n",
      "train loss:0.10880376216707928\n",
      "train loss:0.04467668047379877\n",
      "train loss:0.11917454177436282\n",
      "train loss:0.05537253613232833\n",
      "train loss:0.08438623323515908\n",
      "train loss:0.0862270560997736\n",
      "train loss:0.09603532752348912\n",
      "train loss:0.18215031752109984\n",
      "train loss:0.1614626878298782\n",
      "train loss:0.19775493052650586\n",
      "train loss:0.11060168884538792\n",
      "train loss:0.09233018211704065\n",
      "train loss:0.09694309283466625\n",
      "train loss:0.08402153093978919\n",
      "train loss:0.1265573963171736\n",
      "train loss:0.1054504506940805\n",
      "train loss:0.1394116993628481\n",
      "train loss:0.08122567309649842\n",
      "train loss:0.061652720276511294\n",
      "train loss:0.07616737326087053\n",
      "train loss:0.06972949643329288\n",
      "train loss:0.07453032583064965\n",
      "train loss:0.12462976469682241\n",
      "train loss:0.14286206734615595\n",
      "=== epoch:11, train acc:0.964, test acc:0.933 ===\n",
      "train loss:0.12820440919781176\n",
      "train loss:0.10088335833574319\n",
      "train loss:0.03759975670711225\n",
      "train loss:0.08271504967936756\n",
      "train loss:0.07037081827921869\n",
      "train loss:0.059759455296367\n",
      "train loss:0.03680813300006037\n",
      "train loss:0.06360305406098872\n",
      "train loss:0.17757490449868885\n",
      "train loss:0.08490785035251838\n",
      "train loss:0.15563882093499487\n",
      "train loss:0.08242468328913059\n",
      "train loss:0.06117930230317744\n",
      "train loss:0.13650779828233825\n",
      "train loss:0.10593904594882111\n",
      "train loss:0.030595599901663082\n",
      "train loss:0.09633063917097058\n",
      "train loss:0.16041209719023308\n",
      "train loss:0.1111744067466203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.15571780772466665\n",
      "train loss:0.05827242930189328\n",
      "train loss:0.10266379117909821\n",
      "train loss:0.05616630208612622\n",
      "train loss:0.1072228305093741\n",
      "train loss:0.08296786859064981\n",
      "train loss:0.10227719835329978\n",
      "train loss:0.18834292572820024\n",
      "train loss:0.127367418857731\n",
      "train loss:0.10480646971073758\n",
      "train loss:0.13989874491671522\n",
      "train loss:0.037599616467122086\n",
      "train loss:0.17683721772194338\n",
      "train loss:0.12151558160404441\n",
      "train loss:0.15664859060779832\n",
      "train loss:0.09077901164580107\n",
      "train loss:0.07413097938173031\n",
      "train loss:0.07586396008500781\n",
      "train loss:0.08221723106956016\n",
      "train loss:0.04014951381924876\n",
      "train loss:0.0732687917402625\n",
      "train loss:0.0828127612303715\n",
      "train loss:0.07943362422433187\n",
      "train loss:0.07837383151303427\n",
      "train loss:0.08334143027696708\n",
      "train loss:0.08472418195866255\n",
      "train loss:0.08081507365716414\n",
      "train loss:0.060019146653707646\n",
      "train loss:0.04908628232300197\n",
      "train loss:0.10979603279186576\n",
      "train loss:0.06143883482731715\n",
      "=== epoch:12, train acc:0.967, test acc:0.945 ===\n",
      "train loss:0.07575567294992938\n",
      "train loss:0.06365734685634784\n",
      "train loss:0.09914081488177126\n",
      "train loss:0.07187172728395845\n",
      "train loss:0.08936898902343529\n",
      "train loss:0.11537836932495034\n",
      "train loss:0.0939397465587365\n",
      "train loss:0.04645412513365904\n",
      "train loss:0.07760927134980172\n",
      "train loss:0.1058060035167628\n",
      "train loss:0.0985140556161523\n",
      "train loss:0.04423582194434868\n",
      "train loss:0.05582892518017169\n",
      "train loss:0.04854791735270037\n",
      "train loss:0.04591260911145478\n",
      "train loss:0.07004611275766566\n",
      "train loss:0.09848163100743611\n",
      "train loss:0.17641129402569333\n",
      "train loss:0.05103114745744615\n",
      "train loss:0.062236139094977626\n",
      "train loss:0.07135516296578545\n",
      "train loss:0.11768978634419425\n",
      "train loss:0.01985436733829092\n",
      "train loss:0.06901083309551698\n",
      "train loss:0.07678248252889583\n",
      "train loss:0.06364252187511099\n",
      "train loss:0.042577588022278394\n",
      "train loss:0.09451347264257841\n",
      "train loss:0.10772415742439195\n",
      "train loss:0.07178241925865733\n",
      "train loss:0.14109755400180723\n",
      "train loss:0.047529454070792135\n",
      "train loss:0.04598235111588185\n",
      "train loss:0.0773680776228056\n",
      "train loss:0.09588351450018762\n",
      "train loss:0.13452946256784484\n",
      "train loss:0.06102884421625764\n",
      "train loss:0.03218784393671506\n",
      "train loss:0.03865640994379037\n",
      "train loss:0.058128839913124244\n",
      "train loss:0.1536786831534109\n",
      "train loss:0.09055534352646674\n",
      "train loss:0.06777175659986488\n",
      "train loss:0.0306168090942988\n",
      "train loss:0.04511297113257764\n",
      "train loss:0.08171986656347491\n",
      "train loss:0.13979964879858597\n",
      "train loss:0.03737139752954666\n",
      "train loss:0.07052568313602176\n",
      "train loss:0.08217048175421972\n",
      "=== epoch:13, train acc:0.965, test acc:0.947 ===\n",
      "train loss:0.181214165090636\n",
      "train loss:0.04369665695492055\n",
      "train loss:0.11873112935274603\n",
      "train loss:0.03622345050749333\n",
      "train loss:0.12772932846011195\n",
      "train loss:0.05898703050175306\n",
      "train loss:0.10695608717407952\n",
      "train loss:0.10437084395145545\n",
      "train loss:0.08710521626091007\n",
      "train loss:0.08278513573899567\n",
      "train loss:0.11982491210185695\n",
      "train loss:0.06326164565852514\n",
      "train loss:0.06439169768658945\n",
      "train loss:0.05781199020739703\n",
      "train loss:0.05120345713447791\n",
      "train loss:0.11953525457216008\n",
      "train loss:0.1351244412769382\n",
      "train loss:0.06711457980461552\n",
      "train loss:0.10155459649093425\n",
      "train loss:0.07320465636132414\n",
      "train loss:0.06694052206678476\n",
      "train loss:0.0840096258764768\n",
      "train loss:0.06322744858503583\n",
      "train loss:0.05299388047180813\n",
      "train loss:0.04889768088923183\n",
      "train loss:0.09067237200675077\n",
      "train loss:0.046671231720047154\n",
      "train loss:0.1210937373541498\n",
      "train loss:0.0747830547860039\n",
      "train loss:0.06326606634633401\n",
      "train loss:0.0684595740709562\n",
      "train loss:0.042164355278754846\n",
      "train loss:0.05219425724216026\n",
      "train loss:0.112468211965892\n",
      "train loss:0.07106750889292326\n",
      "train loss:0.041506608978274115\n",
      "train loss:0.03278184669331201\n",
      "train loss:0.11836715662709349\n",
      "train loss:0.05559153670506672\n",
      "train loss:0.029102593291457132\n",
      "train loss:0.03533573196989791\n",
      "train loss:0.046309426295667526\n",
      "train loss:0.07512315027109837\n",
      "train loss:0.03384841323337959\n",
      "train loss:0.07946212784042271\n",
      "train loss:0.11286269897825488\n",
      "train loss:0.04876767125697744\n",
      "train loss:0.0794455836254897\n",
      "train loss:0.04781345269184195\n",
      "train loss:0.050011730348359366\n",
      "=== epoch:14, train acc:0.973, test acc:0.948 ===\n",
      "train loss:0.0787115698758975\n",
      "train loss:0.07820904139594415\n",
      "train loss:0.11217458729380145\n",
      "train loss:0.060683706526094186\n",
      "train loss:0.040126730733345314\n",
      "train loss:0.07006938143251334\n",
      "train loss:0.10603003235648505\n",
      "train loss:0.05691582195507441\n",
      "train loss:0.023733448326016208\n",
      "train loss:0.05090548717098124\n",
      "train loss:0.06294148232144509\n",
      "train loss:0.10909653714025642\n",
      "train loss:0.08115044961595132\n",
      "train loss:0.0512911270059198\n",
      "train loss:0.10868208208024795\n",
      "train loss:0.05749093863997616\n",
      "train loss:0.11693682206802204\n",
      "train loss:0.06677581119298588\n",
      "train loss:0.0876739834680962\n",
      "train loss:0.04254799178616007\n",
      "train loss:0.039861791733014657\n",
      "train loss:0.019550061430969854\n",
      "train loss:0.04997142390764189\n",
      "train loss:0.07375233498868307\n",
      "train loss:0.053266834017797866\n",
      "train loss:0.04540134285940538\n",
      "train loss:0.05164357531449716\n",
      "train loss:0.05389419515422597\n",
      "train loss:0.06560776878068858\n",
      "train loss:0.06511067101728119\n",
      "train loss:0.06004343103801723\n",
      "train loss:0.05549946847339649\n",
      "train loss:0.06556780340703641\n",
      "train loss:0.0532656452846519\n",
      "train loss:0.037496381296623034\n",
      "train loss:0.02996008151836176\n",
      "train loss:0.07387170469717876\n",
      "train loss:0.05480404080893566\n",
      "train loss:0.058406764113124406\n",
      "train loss:0.026530164791355983\n",
      "train loss:0.031891399017681905\n",
      "train loss:0.07703037660113952\n",
      "train loss:0.10275676009799677\n",
      "train loss:0.03995074399886529\n",
      "train loss:0.08017814425453315\n",
      "train loss:0.06398650525380084\n",
      "train loss:0.08991363187586822\n",
      "train loss:0.04498359290903625\n",
      "train loss:0.07229955546785874\n",
      "train loss:0.06690435924836481\n",
      "=== epoch:15, train acc:0.978, test acc:0.949 ===\n",
      "train loss:0.016671056116356615\n",
      "train loss:0.05987541382219992\n",
      "train loss:0.02301394854902217\n",
      "train loss:0.0676409556394215\n",
      "train loss:0.01580884112791829\n",
      "train loss:0.048822922407281304\n",
      "train loss:0.07290544455704084\n",
      "train loss:0.19471297764526466\n",
      "train loss:0.12092494954079239\n",
      "train loss:0.04789317284264606\n",
      "train loss:0.04226562631689063\n",
      "train loss:0.029202037149984362\n",
      "train loss:0.023333759800222384\n",
      "train loss:0.1309254269654287\n",
      "train loss:0.06760859137690498\n",
      "train loss:0.06001928274272737\n",
      "train loss:0.04316918350761553\n",
      "train loss:0.04241237847482517\n",
      "train loss:0.047750937966777825\n",
      "train loss:0.038247315861345894\n",
      "train loss:0.10498542481416409\n",
      "train loss:0.055679826003132725\n",
      "train loss:0.05991595032103613\n",
      "train loss:0.05981993721774332\n",
      "train loss:0.05586116967891917\n",
      "train loss:0.023153216263736728\n",
      "train loss:0.03845941749371353\n",
      "train loss:0.038952736466997984\n",
      "train loss:0.12358780280643106\n",
      "train loss:0.02086073073092164\n",
      "train loss:0.025255533927996458\n",
      "train loss:0.06725250644216133\n",
      "train loss:0.020869563629846075\n",
      "train loss:0.10136498876320438\n",
      "train loss:0.05776713979192454\n",
      "train loss:0.040021724375368224\n",
      "train loss:0.07299723580613757\n",
      "train loss:0.06942805919053371\n",
      "train loss:0.02587672434403424\n",
      "train loss:0.03151400249212635\n",
      "train loss:0.03702067376758772\n",
      "train loss:0.059953419350667156\n",
      "train loss:0.050153270955472735\n",
      "train loss:0.014145114197847286\n",
      "train loss:0.03197301866195564\n",
      "train loss:0.033430359334052466\n",
      "train loss:0.05313365013535264\n",
      "train loss:0.046148807214067215\n",
      "train loss:0.02310085420086714\n",
      "train loss:0.0224116587070751\n",
      "=== epoch:16, train acc:0.987, test acc:0.95 ===\n",
      "train loss:0.040523056016639705\n",
      "train loss:0.030028914208217013\n",
      "train loss:0.046739111137015836\n",
      "train loss:0.030857138526064247\n",
      "train loss:0.028017921182861966\n",
      "train loss:0.02021668157987327\n",
      "train loss:0.0539810655232297\n",
      "train loss:0.10016306813092896\n",
      "train loss:0.07473436204949296\n",
      "train loss:0.04229176794113866\n",
      "train loss:0.027879332202602233\n",
      "train loss:0.05238955410433649\n",
      "train loss:0.06031291386911734\n",
      "train loss:0.05119015769550356\n",
      "train loss:0.08111491118947173\n",
      "train loss:0.014043598269011907\n",
      "train loss:0.02954809033656989\n",
      "train loss:0.0282268804087494\n",
      "train loss:0.012263895880382469\n",
      "train loss:0.02080391916291627\n",
      "train loss:0.05192998127179733\n",
      "train loss:0.07717432779749386\n",
      "train loss:0.014923492818656615\n",
      "train loss:0.11056629148638301\n",
      "train loss:0.05463133372277139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.04016524580767424\n",
      "train loss:0.07829271115606877\n",
      "train loss:0.05731743080438207\n",
      "train loss:0.0276138126027094\n",
      "train loss:0.02603581277119785\n",
      "train loss:0.03877878815715824\n",
      "train loss:0.027283713558599234\n",
      "train loss:0.0761125113516903\n",
      "train loss:0.016571129027479453\n",
      "train loss:0.02925876716977879\n",
      "train loss:0.018365300336264635\n",
      "train loss:0.018240105169918294\n",
      "train loss:0.04361803985664607\n",
      "train loss:0.08038423110202636\n",
      "train loss:0.02121698696160128\n",
      "train loss:0.07246671140142788\n",
      "train loss:0.02685544935783339\n",
      "train loss:0.0406170372905287\n",
      "train loss:0.01873982571335231\n",
      "train loss:0.03636512393937171\n",
      "train loss:0.022722265400535888\n",
      "train loss:0.03404992681380258\n",
      "train loss:0.029168252776552196\n",
      "train loss:0.015002305944363825\n",
      "train loss:0.05756664015143171\n",
      "=== epoch:17, train acc:0.982, test acc:0.95 ===\n",
      "train loss:0.10819469271204409\n",
      "train loss:0.02144834483946862\n",
      "train loss:0.020871199397487383\n",
      "train loss:0.014011987851683833\n",
      "train loss:0.008703309870712312\n",
      "train loss:0.04792006731406356\n",
      "train loss:0.03366010897294524\n",
      "train loss:0.060908498172013534\n",
      "train loss:0.049763138625697347\n",
      "train loss:0.03487263073016972\n",
      "train loss:0.10076762391322833\n",
      "train loss:0.02614947665128691\n",
      "train loss:0.030441884903714293\n",
      "train loss:0.03633098094939506\n",
      "train loss:0.03741353914084273\n",
      "train loss:0.038898243304675686\n",
      "train loss:0.09886803119356255\n",
      "train loss:0.060468405921569775\n",
      "train loss:0.0349882220237324\n",
      "train loss:0.05398108199694593\n",
      "train loss:0.08529114402025133\n",
      "train loss:0.0871537965683275\n",
      "train loss:0.02610360859464471\n",
      "train loss:0.03935692717381973\n",
      "train loss:0.06720773484235881\n",
      "train loss:0.03178733612395172\n",
      "train loss:0.05246776483515185\n",
      "train loss:0.04767599394342736\n",
      "train loss:0.06702663631809933\n",
      "train loss:0.04910680161211773\n",
      "train loss:0.017732850725436948\n",
      "train loss:0.018382765937368126\n",
      "train loss:0.032370627674844464\n",
      "train loss:0.036042064886179205\n",
      "train loss:0.019492837519132417\n",
      "train loss:0.06359308565756291\n",
      "train loss:0.024847897008465392\n",
      "train loss:0.026297428098702655\n",
      "train loss:0.01665986288563037\n",
      "train loss:0.07844957601046147\n",
      "train loss:0.053971623126119236\n",
      "train loss:0.029489349970418955\n",
      "train loss:0.052397119094988114\n",
      "train loss:0.05664491507070133\n",
      "train loss:0.01929318659663742\n",
      "train loss:0.035540458356402516\n",
      "train loss:0.021426026380944877\n",
      "train loss:0.01485898178816389\n",
      "train loss:0.03802037955229403\n",
      "train loss:0.03939510463618551\n",
      "=== epoch:18, train acc:0.984, test acc:0.957 ===\n",
      "train loss:0.08340971443750819\n",
      "train loss:0.01864415717017133\n",
      "train loss:0.08569869372161071\n",
      "train loss:0.01976064605224995\n",
      "train loss:0.024599710509785776\n",
      "train loss:0.033634596333296365\n",
      "train loss:0.0793562536154107\n",
      "train loss:0.039968359001238477\n",
      "train loss:0.023036187134719645\n",
      "train loss:0.043868906451589404\n",
      "train loss:0.012322551895518263\n",
      "train loss:0.01669090764420448\n",
      "train loss:0.048023729088068255\n",
      "train loss:0.07269810119194368\n",
      "train loss:0.03300484641553502\n",
      "train loss:0.029294365379257475\n",
      "train loss:0.023981101282487163\n",
      "train loss:0.03436272713209762\n",
      "train loss:0.06426728744761726\n",
      "train loss:0.018094995767705\n",
      "train loss:0.018328116484893905\n",
      "train loss:0.0876001091774658\n",
      "train loss:0.016614628457971275\n",
      "train loss:0.019447212080112887\n",
      "train loss:0.0134372106480865\n",
      "train loss:0.018176925737167348\n",
      "train loss:0.009460056499879872\n",
      "train loss:0.022113857340400656\n",
      "train loss:0.029151403482466656\n",
      "train loss:0.02371309963631041\n",
      "train loss:0.029557392699923805\n",
      "train loss:0.01931074135681327\n",
      "train loss:0.030524522759683363\n",
      "train loss:0.023160862419605927\n",
      "train loss:0.03271666831160102\n",
      "train loss:0.049782248496832784\n",
      "train loss:0.05509560981914551\n",
      "train loss:0.021322767332707188\n",
      "train loss:0.019887506317019882\n",
      "train loss:0.034118445990988525\n",
      "train loss:0.01841748673454426\n",
      "train loss:0.023510203917271832\n",
      "train loss:0.027102177452267773\n",
      "train loss:0.13023935573836481\n",
      "train loss:0.02307157928853567\n",
      "train loss:0.03836448499208461\n",
      "train loss:0.008767800846219625\n",
      "train loss:0.016730940452709127\n",
      "train loss:0.01264573432088909\n",
      "train loss:0.024218242541040463\n",
      "=== epoch:19, train acc:0.981, test acc:0.953 ===\n",
      "train loss:0.01940467081766098\n",
      "train loss:0.02025307137884034\n",
      "train loss:0.029064009981029662\n",
      "train loss:0.02087611966878593\n",
      "train loss:0.07218787125998576\n",
      "train loss:0.02388821068674072\n",
      "train loss:0.02086089344086785\n",
      "train loss:0.07786311023137002\n",
      "train loss:0.014625000503286674\n",
      "train loss:0.022341839370419718\n",
      "train loss:0.024791224084926337\n",
      "train loss:0.022335314837644383\n",
      "train loss:0.029654345419308704\n",
      "train loss:0.049170516411033274\n",
      "train loss:0.016577889466468165\n",
      "train loss:0.01709386601796968\n",
      "train loss:0.03594119270805678\n",
      "train loss:0.01404313139198105\n",
      "train loss:0.021276932551591732\n",
      "train loss:0.026085711900045484\n",
      "train loss:0.004447322332484973\n",
      "train loss:0.034355765827540456\n",
      "train loss:0.03790548754518484\n",
      "train loss:0.02361815329883875\n",
      "train loss:0.011369893340483833\n",
      "train loss:0.017415351933055986\n",
      "train loss:0.014902789245575941\n",
      "train loss:0.013450922301980422\n",
      "train loss:0.0553878071059858\n",
      "train loss:0.013189460239414941\n",
      "train loss:0.035605689352794456\n",
      "train loss:0.019518027198699103\n",
      "train loss:0.02805788338829009\n",
      "train loss:0.010206796241994888\n",
      "train loss:0.03915522704582927\n",
      "train loss:0.04327090794205191\n",
      "train loss:0.009677235890656215\n",
      "train loss:0.02687491575018162\n",
      "train loss:0.01027125217450734\n",
      "train loss:0.046858606791526015\n",
      "train loss:0.07263393264946735\n",
      "train loss:0.03485476000167388\n",
      "train loss:0.019618624640185947\n",
      "train loss:0.02104576256158817\n",
      "train loss:0.020015094618670775\n",
      "train loss:0.024096428528474535\n",
      "train loss:0.02515200060917719\n",
      "train loss:0.03154887662292408\n",
      "train loss:0.047415909043092494\n",
      "train loss:0.03474844066131446\n",
      "=== epoch:20, train acc:0.993, test acc:0.96 ===\n",
      "train loss:0.02402450101406874\n",
      "train loss:0.02536927706758117\n",
      "train loss:0.017842441444061476\n",
      "train loss:0.012990868083665947\n",
      "train loss:0.015660998102205663\n",
      "train loss:0.008947707823320375\n",
      "train loss:0.027154232944994874\n",
      "train loss:0.0354600201726461\n",
      "train loss:0.015408571971103808\n",
      "train loss:0.017040605820220947\n",
      "train loss:0.0261703438236626\n",
      "train loss:0.01196352961921761\n",
      "train loss:0.021554172918015847\n",
      "train loss:0.015284414164457005\n",
      "train loss:0.02648851926144957\n",
      "train loss:0.012888633271880971\n",
      "train loss:0.008496377763498889\n",
      "train loss:0.013917460764784834\n",
      "train loss:0.009631066340292595\n",
      "train loss:0.02261534061271542\n",
      "train loss:0.012152247262784157\n",
      "train loss:0.010909423702481068\n",
      "train loss:0.04744879403786497\n",
      "train loss:0.04742783251203378\n",
      "train loss:0.008106937080104742\n",
      "train loss:0.007500549018641134\n",
      "train loss:0.014653776614792382\n",
      "train loss:0.010447621242282212\n",
      "train loss:0.0050163810995202665\n",
      "train loss:0.0211405287464461\n",
      "train loss:0.023754714299932665\n",
      "train loss:0.04687478797427283\n",
      "train loss:0.023025747761068634\n",
      "train loss:0.04249199154449612\n",
      "train loss:0.0075742556309224725\n",
      "train loss:0.026001413116161446\n",
      "train loss:0.007676595512636167\n",
      "train loss:0.023010710173124126\n",
      "train loss:0.020455747717268165\n",
      "train loss:0.07394126298892142\n",
      "train loss:0.044173458608461946\n",
      "train loss:0.011944300673151713\n",
      "train loss:0.020193453912942985\n",
      "train loss:0.010368999853212412\n",
      "train loss:0.01943536229797482\n",
      "train loss:0.02378338219113612\n",
      "train loss:0.012796084293500513\n",
      "train loss:0.0368880455813264\n",
      "train loss:0.022181175476374042\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.956\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 保存参数\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 绘制图形\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3bb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
